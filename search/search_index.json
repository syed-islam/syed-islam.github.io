{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Syed's Home","text":""},{"location":"#life-at-present","title":"Life at present","text":"<p> I am currently an Executive Director (Principal Cybersecurity Architect) working in Cyber Security and Technology Controls for JP Morgan Chase. I am also a member of the Industrial Advisory Board for Queen Mary University. </p> <p>My wife and I have two beautiful boys - Arzaan and Afaan who take up nearly all our time outside of work. </p> <p>If you would like to get in touch with me, please use one of the following email addresses or one of the social media handles:</p> <ul> <li><code>syed.islam.10[at]ucl.ac.uk</code></li> <li><code>syed.islam.ucl[at]gmail.com</code></li> </ul>"},{"location":"#work-experience","title":"Work Experience","text":"<p>I previously worked as/at:</p> <ul> <li>Vice President (Data Architect) at JP Morgan Chase.</li> <li>Associate (Information Architect) at JP Morgan Chase.</li> <li>Programme Leader &amp; Senior Lecturer (Associate Professor) at University of East London.</li> <li>Chief Architect at AvasUnion UK.</li> <li>Research Associate at CHIME, University College London.</li> <li>Research Associate at CREST, University College London.</li> <li>Integration Engineer at Intercontinental Exchange (ICE Link) London.</li> <li>Graduate Teaching Assistant at University College London. </li> <li>Graduate Teaching Assistant at King's College London.</li> <li>Consultant Ornate i System, London.</li> <li>Consultant at KernelSoft, Dhaka. </li> <li>Lecturer at Brac University, Dhaka.</li> </ul>"},{"location":"#education","title":"Education","text":"<p>I previously studied:</p> <ul> <li>PhD. Advanced Computing Research at University College London.</li> <li>MSc. Advanced Software Engineering at King's College London.</li> <li>BSc. Computer Science and Computer Networking at London Metropolitan University.</li> </ul>"},{"location":"#certifications","title":"Certifications","text":"<p>I also have the following certifications:</p> <ul> <li>AWS Solutions Architect Associate (SAA) </li> <li>Cisco Certified Network Associate (CCNA)</li> <li>Graduate Certificate in Academic Practices (GCAP)</li> <li>Postgraduate Certificate in Learning and Teaching in Higher Education</li> </ul>"},{"location":"publication/","title":"Publications","text":""},{"location":"publication/#journal-articles","title":"Journal Articles","text":"<ul> <li>Kingsley Okoye, Syed Islam, Usman Naeem and MHD Saeed Sharif Semantic-based process mining technique for annotation and modelling of domain processes.. The International Journal of Innovative Computing, Information and Control, January 2020. [PDF] \u00a9 Elsevier, 2020. DOI:</li> <li>Seongmin Lee, David Binkley, Nicolas Gold, Syed Islam, Jens Krinke and Shin Yoo: Evaluating lexical approximation of program dependence. Journal of Systems and Software, [PDF] \u00a9 Elsevier, 2020. DOI: 10.1016/j.jss.2019.110459</li> <li>David Binkley, Nicolas Gold, Syed Islam, Jens Krinke and Shin Yoo: A Comparison of Tree- and Line-Oriented Observational Slicing. Empirical Software Engineering, 1--37 [PDF] \u00a9 Springer, 2019. DOI: 10.1007/s10664-018-9675-9</li> <li>Kingsley Okoye, Usman Naeem, Syed Islam, Abdel-Rahman H. Tawil &amp; Elyes Lamine: Process Models Discovery and Traces Classification: a Fuzzy-BPMN Mining Approach.Journal of International Technology and Information Management (JITIM) - online, 2018. [PDF] \u00a9 IIMA, 2018. ISSN:</li> <li>Isibor Ihianle, Usman Naeem, Syed Islam and Abdel-Rahman Tawil: A Hybrid approach to Recognising Activities of Daily. Informatics - Special Issue : Sensor-Based Activity Recognition and Interaction, 2018 [PDF] \u00a9 MDPI Press, 2018. DOI:</li> <li>Isibor Ihianle, Usman Naeem, Syed Islam and Abdel-Rahman Tawil: Recognising Activities of Daily Living from Patterns of Object Use. International Journal of Hybrid intelligent Systems (IJHIS), 14(3): 2018. [PDF] \u00a9 IOS Press, 2018. DOI:</li> <li>Pusp Joshi, Shareeful Islam and Syed Islam: A Framework for Cloud Based E-Government from the Perspective of Developing Countries. Future Internet, 80(9): 2017. [PDF] \u00a9 MDPI, 2017. DOI: 10.3390/fi9040080 </li> <li>Kingsley Okoye, Usman Naeem and Syed Islam: Semantic Fuzzy Mining: Enhancement of process models and event logs analysis from Syntactic to Conceptual Level. International Journal of Hybrid Intelligent System, 14(1): 2017. [PDF] \u00a9 IOS Press, 2017. DOI: 10.3233/HIS-170243</li> <li>Muhammad Ehatisham-ul-Haq, Muhammad Awais Azam, Jonathan Loo, Kai Shuang, Syed Islam, Usman Naeem and Yasar Amin: Authentication of Smartphone Users Based on Activity Recognition and Mobile Sensing. Sensors, 17(9): 2017. [PDF] \u00a9 MDPI AG, 2017. DOI: 10.3390/s17092043</li> <li>B. de Bono, M. Helvensteijn, N. Kokash, I. Martorelli, D. Sarwar, S. Islam, P. Grenon and P. Hunter: Requirements for the formal representation of pathophysiology mechanisms by clinicians. Interface Focus, 6(2): 2016, p.20150099. [PDF] \u00a9 The Royal Society Publishing, 2016. DOI: 10.1098/rsfs.2015.0099</li> <li>S. Islam, J. Krinke, D. Binkley and M. Harman: Coherent Clusters in Source Code. Journal of Systems and Software, 88: February 2014. [PDF] \u00a9 Open Access, 2014. DOI: 10.1016/j.jss.2013.07.040</li> <li>D. Binkley, N. Gold, M. Harman, S. Islam, J. Krinke and Z. Li: Efficient identification of linchpin vertices in dependence clusters. ACM Transactions on Programming Languages and Systems (TOPLAS), 35(2): July 2013. [PDF] \u00a9 ACM, 2013. DOI: 10.1145/2491522.2491524</li> <li>D. Binkley, M. Harman, Y. Hassoun, S. Islam, Z. Li: Assessing the Impact of Global Variables on Program Dependence and Dependence Clusters. Journal of Systems and Software, 83(1) January, 2010. [PDF] \u00a9 Elsevier, 2010. DOI: 10.1016/j.jss.2009.03.03 </li> </ul>"},{"location":"publication/#conferences","title":"Conferences","text":"<ul> <li>Usman Naeem, Syed Islam and Arish Siddiqui: Improving Student Engagement and Performance in Computing Final Year Projects. IEEE International Conference on Teaching, Assessment, and Learning for Engineering, December 2019. [PDF] \u00a9 IEEE, 2019.</li> <li>Mahin Talukder, Syed Islam and Paolo Falcarin: Analysis of Obfuscated Code with Program Slicing. Cyber Science, June 2019. [PDF] \u00a9 IEEE, 2019. DOI: </li> <li>Usman Naeem, Syed Islam and Arish Siddiqui: An Effective Framework for Enhancing Student Engagement and Performance in Final Year Projects. IEEE Global Engineering Education Conference (EDUCON2019), April 2019. [PDF] \u00a9 IEEE, 2019. DIO: </li> <li>Syed Islam, Kingsley Okoye, Usman Naeem, Saeed Sharif, Muhammad Awais Azam and Amin Karami: The Application of a Semantic-Based Process Mining Framework on a Learning Process Domain. Intelligent Systems Conference (IntelliSys) 2018, September 2018. [PDF] \u00a9 IEEE, 2018. DOI:</li> <li>Isibor IHIANLE, Syed Islam, Usman Naeem, Saeed Sharif, Muhammad Awais Azam and Amin Karami: Intelligent Recognition of Activities of Daily Living from Patterns of Object Use. Intelligent Systems Conference (IntelliSys) 2018, September 2018. [PDF] \u00a9 IEEE, 2018. DOI:</li> <li>Saeed Sharif, Usman Naeem, Syed Islam and Amin Karami,: Functional Connectivity Evaluation for Infant EEG Signals Based on Artificial Neural Network. Intelligent Systems Conference (IntelliSys) 2018, September 2018. [PDF] \u00a9 IEEE, 2018. DOI:</li> <li>Isibor Kennedy Ihianle, Usman Naeem and Syed Islam: Knowledge Driven Activity Recognition from Patterns of Object Use. 5th Activity Monitoring by Multiple Distributed Sensing Workshop AMMDS 2017, September 2017. [PDF] \u00a9 BVMC, 2017. DOI:</li> <li>Dave Binkley, Nicolas Gold, Mark Harman, Syed Islam, Jens Krinke and Shin Yoo: Tree-Oriented vs. Line-Oriented Observation-Based Slicing. International Working Conference on Source Code Analysis and Manipulation, September 2017. [PDF] \u00a9 IEEE, 2017. DOI: Best Paper Award</li> <li>Kingsley Okoye, Usman Naeem, Syed Islam, Abdel-Rahman H. Tawil and Elyes Lamine: Process Models Discovery: A Fuzzy-BPMN Mining Approach. In Smart Systems for Complex Problems 2017, Chapter: 13, pp. 17, 2017. [PDF] \u00a9 IIMA/ICITED Joint Conference, 2017. ISBN: 978-1-64136-046-3</li> <li>Syed Islam, Mhd Saeed Sharif, Usman Naeem and James Geehan: SignalSense - Towards Quality Service. International Workshop on Intelligent Personal Support of Human Behavior, September 2017. [PDF] \u00a9 ACM, 2017. DOI:</li> <li>Syed Islam, Usman Naeem, Mhd Saeed Sharif and Arnold Dovnarovic: CrimeSafe - Helping you stay safe. International Workshop on Intelligent Personal Support of Human Behavior, September 2017. [PDF] \u00a9 ACM, 2017. DOI:</li> <li>Usman Naeem, Syed Islam, Mhd Saeed Sharif, Sergey Sudakov and M. Awais Azam: Taskification - Gamification of Tasks. International Workshop on Intelligent Personal Support of Human Behavior, September 2017. [PDF] \u00a9 ACM, 2017. DOI:</li> <li>Isibor Kennedy Ihianle, Usman Naeem, and Syed Islam: Ontology-Driven Activity Recognition from Patterns of Object Use. International Workshop on Intelligent Personal Support of Human Behavior, September 2017. [PDF] \u00a9 ACM, 2017. DOI:</li> <li>Nicolas Gold, David Binkley, Mark Harman, Syed Islam, Jens Krinke and Shin Yoo: Generalized Observational Slicing for Tree-Represented Modelling Languages. Symposium on the Foundations of Software Engineering September 2017. [PDF] \u00a9 ACM, 2017. DOI:</li> <li>Nigar Jebraeil, Adel Noureddine, Joseph Doyle, Syed Islam and Rabih Bashroush: gUML: Reasoning about Energy at Design Time by Extending UML Deployment Diagrams with Data Centre Contextual Information. IEEE SERVICES2017 Emerging Technology Track June 2017. [PDF] \u00a9 IEEE, 2017. DOI:</li> <li>Joseph Doyle, Syed Islam, Rabih Bashroush and Donal O'Mahony: Cloud Strife: Expanding the Horizons of Cloud Gaming Services. IEEE Services 2017 Research Track June 2017. [PDF] \u00a9 IEEE, 2017. DOI:</li> <li>Kingsley Okoye, Abdel-Rahman Tawil, Usman Naeem, Syed Islam and Elyes Lamine: Using Semantic-based Approach to Manage Perspectives of Process Mining: Application on Improving Learning Process Domain Data. IEEE International Conference on Big Data (Big Data), December 2016. [PDF] \u00a9 IEEE, 2016. DOI:</li> <li>Kingsley Okoye, Abdel-Rahman Tawil, Usman Naeem, Syed Islam and Elyes Lamine: Semantic-based Model Analysis towards Enhancing Information Values of Process Mining: Case Study of Learning Process Domain. International Conference on Soft Computing and Pattern Recognition, Vellore, India, December 2016. [PDF] \u00a9 Springer, 2016. DOI:</li> <li>Yibiao Yang, Mark Harman, Jens Krinke, Syed Islam, David Binkley, Yuming Zhou, Baowen Xu: An Empirical Study on Dependence Clusters for Effort-Aware Fault-Proneness Prediction. International Conference on Automated Software Engineering, Singapore, September 2016. [PDF] \u00a9 ACM, 2016. DOI:</li> <li>Umar Ismail, Syed Islam, Shareeful Islam: Towards Cloud Security Monitoring: A Case Study. Cybersecurity and Cyberforensics Conference, Jordan, August 2016. [PDF] \u00a9 IEEE, 2016. DOI:</li> <li>Bernard de Bono, Michiel Helvensteijn, Natallia Kokash, Irene Martorelli,Dewan Sarwar, Syed Islam, Pierre Grenon, S. Randall Thomas, Peter Hunter: ApiNATOMY: Physiology knowledge integration over circuitboards of functional anatomy. Virtual Physiological Human Conference (VPH), Amsterdam, Netherlands, September 2016. [PDF] \u00a9 VPH, 2016. DOI:</li> <li>Adel Noureddine, Syed Islam and Rabih Bashroush: Jolinar: Analysing the energy footprint of Software Applications. International Symposium on Software Testing and Analysis (ISSTA), Saarland, Germany, July 2016. [PDF] \u00a9 ACM, 2016. DOI:</li> <li>Syed Islam, Adel Noureddine and Rabih Bashroush: Measuring Energy Footprint of Software Features. IEEE International Conference on Program Comprehension (ICPC), Austin, USA, May 2016. [PDF] \u00a9 IEEE, 2016. DOI:</li> <li>Syed Islam and David Binkley: PORBS: A Parallel Observation-based Slicer. IEEE International Conference on Program Comprehension (ICPC), Austin, USA, May 2016. [PDF] \u00a9 IEEE, 2016. DOI:</li> <li>D. Binkley, A Beszedes, S. Islam, J. Jasz and B. Vancsics: Uncovering Dependence Clusters and Linchpin Functions. International Conference on Software Maintenance and Evolution (ICSME), Bremen, Germany, 2015. [PDF] \u00a9 IEEE, 2015. DOI:</li> <li>David Binkley, Nicolas Gold, Mark Harman, Syed Islam, Jens Krinke, and Shin Yoo: ORBS and the Limits of Static Slicing. International Working Conference on Source Code Analysis and Manipulation (SCAM), Bremen, Germany, 2015. [PDF] \u00a9 IEEE, 2015. DOI:</li> <li>D. Binkley, N. Gold, M. Harman, S. Islam, J. Krinke, S. Yoo: ORBS: Language-Independent Program Slicing. Foundations of Software Engineering (FSE), Hong Kong, China, November 2014. [PDF] \u00a9 ACM, 2014. DOI: 10.1145/2635868.2635893</li> <li>M. Harman, S. Islam, Y. Jia, L. Minku, F. Sarro and K. Srivisut: Less is more: Temporal fault predictive performance over multiple Hadoop releases. Symposium on Search-Based Software Engineering SSBSE 2014, Fortaleza, Brazil, August 2014. [PDF] \u00a9 Springer International Publishing, 2014. DOI: 10.1007/978-3-319-09940-8_19</li> <li>S. Islam, J. Krinke, D. Binkley: Dependence Clusters Visualization. 5th ACM/IEEE Symposium on Software Visualization, Salt Lake City, Utah, USA, October 2010. [PDF] \u00a9 ACM, 2010. DOI: 10.1145/1879211.1879227</li> <li>S.Islam, J. Krinke, D. Binkley, M. Harman: Coherent Dependence Clusters. ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering, Toronto, Ontario, Canada, June 2010. [PDF] \u00a9 ACM, 2010. DOI: 10.1145/1806672.1806683</li> </ul>"},{"location":"publication/#book-chapters","title":"Book Chapters","text":"<ul> <li>Kingsley Okoye, Syed Islam, and Usman Naeem, 2017; Ontology: Core Process Mining and Querying Enabling Tool. In: C. Thomas, Editor. Ontology in Information Science, ISBN - 978-953-51-5354-2, InTech Publishers. 2018.</li> </ul>"},{"location":"publication/#theses","title":"Theses","text":"<ul> <li>Syed Islam: Coherent Dependence Clusters. Thesis, PhD. Advanced Computing Research, University College London, July 2014. Examiners: Tracy Hall, Brunel University London, and Wolfgang Emmerich, University College London. [PDF] 187 pages.</li> <li>Syed Islam: Dependence Cluster Analysis. Thesis, MSc. Advanced Software Engineering, King's College London, September 2007. Examiners: Mark Harman, King's College London, and Iman Poernomo, King's College London. [PDF] 181 pages.</li> </ul>"},{"location":"academia/admin/","title":"Admin Roles","text":"Role Organization Programme Leader, MSc Computer Science University of East London MSc. Dissertation Coordinator, MSc. Dissertation University of East London Member, School Research Degree Sub-committee, ACE University of East London Deputy Manager, Crest Open Workshop University College London Study Tour Leader, Submarine cable station &amp; Weather stations Brac University Event Organizer, World Cyber Games Brac University Coach, Cisco National Competition Brac University Academic Resource Planner, Department of Computer Science Brac University Faculty Advisor, Brac University Computer Club Brac University Committee Member, Department of Computer Science Brac University"},{"location":"academia/affiliations/","title":"Affiliations to Academic Institutions","text":"<ul> <li>University College London - UCL Computer Science </li> <li>King's College London - Department of Informatics </li> <li>Queen Mary University - School of Electronic Engineering and Computer Science</li> <li>University of East London - School OF Architecture, Computing and Engineering</li> <li>BRAC University - Department of Computer Science and Engineering</li> <li>UCL Centre for Research on Evolution, Search and Testing (CREST)</li> </ul>"},{"location":"academia/cv/","title":"Academic CV","text":"<p>Coming soon...</p>"},{"location":"academia/phd_examination/","title":"PhD Examination","text":""},{"location":"academia/phd_examination/#phd-awarding-viva-voce","title":"PhD Awarding Viva-voce","text":"Candidate Name Thesis Title Year Nigar Jebrailli Ease: Energy Aware Software Engineering 2017 Ahmed Alzahrani Enhanced learning of computer programming in university through collaboration using multi-touch tools 2017"},{"location":"academia/phd_examination/#phd-panels","title":"PhD Panels","text":"Candidate Name Panel Type Year Sanjeewa Jayathilake Annual Review 2017 Chris Moore Annual Review 2017 Abdul Pramanik MPhil/PhD Transfer 2017 Abrar Khan MPhil/PhD Transfer 2017 Comfort Oyede MPhil/PhD Transfer 2016 Eoin woods Annual Review 2016 Daniel Schatz Annual Review 2016 Abel Yeboah-Ofori Annual Review 2016"},{"location":"academia/supervision/","title":"Supervision","text":""},{"location":"academia/supervision/#phd","title":"PhD","text":"Candidate Name Institution Thesis Title Rasha Haridh University of East London Developing an Ontology-Driven Careflow Management System to Support the Process of Homecare for Elderly People with Chronic Diseases. Isibor Ihianle University of East London A Knowledge Driven Activity Recognition Framework Enhanced Using Topic Model. Umar Ismail University of East London Cloud Security Audit And Transparency Framework For Building Enterprise Trust. Kingsley Okoye University of East London A Semantic Rule-Based Approach Supported by Process Mining for Personalised Adaptive Learning. Mahin Talukder University of East London Effective Software Protection. Eoin Woods University of East London Aligning and Integrating Software Architecture Practice and Representation with Software Implementation. Pusp Joshi University of East London A Cloud Based G2G electronic government framework from the perspectives of Nepal. Charles Obi University of East London Assessing security risk for the migration of legacy systems into Cloud in SME\u2019s. Hassan Kassir University of East London Evolution of Software Architecture Sociology."},{"location":"academia/supervision/#professional-doctorate","title":"Professional Doctorate","text":"Candidate Name Institution Thesis Title Charles Obi University of East London Evolutionary Computing for Data Mining and Knowledge Discovery"},{"location":"academia/supervision/#bachelors","title":"Bachelors","text":"Supervisee Name Project Title Completion Year Sarah Hassan Network Assistant Tool 2017 Suleman Khalid Automated Forex Trading System 2017 Suprim Shrestha MyDrone - A mobile application for users to advertise and rent Drones 2017 Cyrille Hounvio Instant Messaging with Automatic Translation 2017 Khalid Owfar NutriPortion 2017 Julian Job Baffoe Ethical and Social Implications on Social Media on Academic Performance 2016 Krystian Blaszkowicz Accessible wireless network sensors using mobile phones 2016 Jerrel Anthony Nicholas Malware Detection 2016 Oyedeji Jimoh Oyetunde Programming Tools for School Children 2016"},{"location":"academia/teaching/","title":"Teaching","text":""},{"location":"academia/teaching/#university-of-east-london","title":"University of East London","text":"<ul> <li>MSc Dissertation</li> <li>Undergraduate Final Year Project</li> <li>Introduction to Computer Systems and Network</li> <li>Computing in Practice</li> <li>Advanced Software Engineering</li> <li>Advanced Programming</li> <li>Introduction to Software Development</li> </ul>"},{"location":"academia/teaching/#university-college-london","title":"University College London","text":"<ul> <li>Algorithmics</li> <li>Tools and Environments</li> </ul>"},{"location":"academia/teaching/#kings-college-london","title":"King's College London","text":"<ul> <li>Programming Applications</li> <li>Data Structures</li> <li>Introduction to Artificial Intelligence</li> <li>Practical Experiences of Programming</li> <li>Programming Practices</li> </ul>"},{"location":"academia/teaching/#brac-university","title":"BRAC University","text":"<ul> <li>Operating Systems</li> <li>Microporcessors</li> <li>Programming for Business</li> <li>Introduction to Programming</li> <li>Data Communications</li> <li>Computer Networks</li> </ul>"},{"location":"academia/teaching/#london-victoria-college-uk","title":"London Victoria College UK","text":"<ul> <li>Information Systems</li> <li>Software Development</li> <li>Computer and Network Technologies</li> <li>Object Oriented Programming</li> <li>Systems Analysis and Design</li> </ul>"},{"location":"academia/teaching/#stamford-college-uk","title":"Stamford College UK","text":"<ul> <li>Information Systems</li> <li>Software Development</li> <li>Computer and Network Technologies</li> <li>Object Oriented Programming</li> <li>Software Engineering</li> <li>Systems Analysis and Design</li> </ul>"},{"location":"awards/awards/","title":"Awards and Achievements","text":"<ul> <li>Nominated for every UEL Student Union Awards 2017-18 category applicable to teaching staff - \"The Imaginative Thinker Award for creative teaching\", \"The Word of Wisdom award for academic advising\", \"The Personal Maestro award for most valuable supervision\", \"The Guiding Star award for most effective feedback\" and \"The Extra Mile award for inspirational teaching\". </li> <li>A student's Thank You message.</li> <li>In Sep '17, our paper on Tree-Oriented vs. Line-Oriented Observation-Based Slicing at the International Working Conference on Source Code Analysis and Manipulation was given the best paper award. Photo</li> <li>In Sep '17, our team (Dr Usman Naeem and I) won the UEL Teaching and Learning Awards for Curriculum Innovation for our re-structuring and innovation of the curriculum for Undergraduate Projects. [Event Videos and Photos]</li> <li>In Apr '16, I received funding to hire research interns under the UEL Internship Program.</li> <li>In Mar '16, I received Microsoft Research Grant ($20,000) for research into improving the techniques for energy measurement of cloud software.</li> <li>In May '14, I received Microsoft Research Grant ($40,000) for research into improving current program slicing techniques and deployment of parallel-ORBS to the cloud infrastructure.</li> <li>University College London (UCL) Graduate Scholarship awarded to match previous ORSAS and King's College Graduate Scholarship to complete my PhD at UCL.</li> <li>Awarded UK Government Overseas Research Student Award Scheme (ORSAS) scholarship to undertake PhD at King's College London. </li> <li>King's College London Graduate Scholarship to undertake funded PhD.</li> <li>Highest teaching evaluation score from student surveys 2008, department of computer science, Brac University.</li> <li>Graduate Scholarship awarded by King's College London during my Masters.</li> <li>Highest Overall Mark, Computer Science Graduate 05/06, London Metropolitan University.</li> <li>Best Overall Mark, Extended Degree 05/06, London Metropolitan University.</li> <li>Multiple Merit Scholarships for outstanding performance during BSc.</li> </ul>"},{"location":"awards/badges/","title":"Badges","text":""},{"location":"awards/event-videos-photo/","title":"University of East London (UEL) Teaching Awards","text":""},{"location":"awards/event-videos-photo/#event-video","title":"Event Video","text":""},{"location":"awards/event-videos-photo/#event-pictures","title":"Event Pictures","text":""},{"location":"notes/aws-dva/introduction/","title":"Introduction","text":"<p>This section will contain my study notes for the AWS Developer Associate Exam. </p>"},{"location":"notes/aws-dva/introduction/#resource-list","title":"Resource List","text":""},{"location":"notes/aws-practitioner/billing-pricing-service/","title":"Billing, Pricing and Support","text":""},{"location":"notes/aws-practitioner/billing-pricing-service/#file-pillars-of-cost-optimization","title":"File Pillars of Cost Optimization","text":"<ul> <li>Right Size - Provision what you need</li> <li>Increase Elasticity - Meet dynamic needs and spikes</li> <li>Pick Right Pricing Model - Choose reserved or etc as needed</li> <li>Match Usage to Storage Cost - Choose right the storage class and performance. </li> <li>Measure and Monitoring - Measure, Monitor via Tagging. Optimize where possible.</li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#tco-total-cost-of-ownership","title":"TCO - Total Cost of Ownership","text":"<ul> <li>Hardware and Software Acquisition</li> <li>Management and Support</li> <li>Communication</li> <li>End-user Expenses</li> <li>Downtime, Training and others</li> </ul> <p>AWS is easier to measure the cost. </p>"},{"location":"notes/aws-practitioner/billing-pricing-service/#pricing-calculator-estimates","title":"Pricing Calculator - Estimates","text":"<ul> <li>Ability to create estimates of cost</li> <li>Steps:</li> <li>Choose Service</li> <li>Choose Region</li> <li>Quick Estimate / Advanced Estimate</li> <li>Specific information about the services.</li> <li>Save and share estimates or create groups of services for estimates.</li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#billing-dashboard","title":"Billing Dashboard","text":"<ul> <li>Gives detailed cost for each AWS Service</li> <li>Credits can be obtained from Training, Non-private Organizations etc. </li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#cost-explorer","title":"Cost Explorer","text":"<ul> <li>Main tool to gather information about all costs and analyze usage in your environment. </li> <li>Deeper insight into the costs, trends, hunt down the max cost drivers</li> <li>Base tool is free, but API requests are charged per request. </li> <li>Hourly view is also payable as it requires more granular data.</li> <li>Possible to create and download reports. </li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#usage-report-cur","title":"Usage Report - CUR","text":"<ul> <li>Needs to be enabled, by default its disabled. </li> <li>Choose S3 bucket where the CUR file lands</li> <li>Choose granularity</li> <li>Data Integration to read it such as - Athena (.parequet), Redshift+Quickshift (.csv) </li> <li>Reports are updated between 1 - 3 times per day.</li> <li>Cost Allocation Tags needs to be activated, once activated both 'User-defined cost allocation tags' and 'AWS-generated cost allocation tags' can be used in the cost explorer. </li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#aws-budget","title":"AWS Budget","text":"<ul> <li>Automatic Notification and Actions when set thresholds are hit. </li> <li>Types:</li> <li>Cost</li> <li>Usage</li> <li>Reservation Budget</li> <li>Saving Plan Budget</li> <li>Actions can be automatic or via workflow approvals</li> <li>Actions be be targeted via IAM policies, SCP policies or directly shut off RDS/EC2. </li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#tagging","title":"Tagging","text":"<ul> <li>Key, Value pairs</li> <li>Should be used groups resources for various business needs. </li> <li>CUR and Cost Explorer can make use of tags explicitly. This has to be done </li> <li>Tags can be updated using: Service Console, AWS CLI and AWS Tag Editor.</li> <li>Keep tagging simple and not overdo. </li> <li>Best to use pre-defined tags rather than make them up on the fly. </li> <li>Setup policies where new resources can't be launched without tags</li> <li>Do regular review of tags.</li> </ul>"},{"location":"notes/aws-practitioner/billing-pricing-service/#amazon-services","title":"Amazon Services","text":"Basic Developer Business Enterprise On-Ramp Enterprise Use case Testing AWS Prod Business Critical Business Critical Trusted Advisor Service Quota / Basic checks Full Full FUll Architectural Guidance General Use-case contextual Consultative review based on application Consultative review based on application Technical Account Manager X X X Pool Designated, proactive monitoring and optimization Training X X X X self-paced labs Account Assistance X X X Concierge Concierge Enhanced Technical Support Business hours emails, 1 contact 24x7, unlimited contacts 24x7, unlimited contacts 24x7, unlimited contacts API X X Yes Yes Yes 3rd party software support Interoperability and troubleshooting Interoperability and troubleshooting Interoperability and troubleshooting Interoperability and troubleshooting Proactive Programs Automation Workflows : <code>AWSSupport</code> Infrastructure Event Management (paid), Automation Workflows : <code>AWSSupport</code> and <code>ASPremimumSupport</code> Infrastructure Event Management (one-per-year), Automation Workflows : <code>AWSSupport</code> and <code>ASPremimumSupport</code> Infrastructure Event Management, Proactive reviews workshop and deep-dives, Automation Workflows : <code>AWSSupport</code> and <code>ASPremimumSupport</code>"},{"location":"notes/aws-practitioner/compute/","title":"Compute","text":""},{"location":"notes/aws-practitioner/compute/#ec2-elastic-cloud-compute","title":"EC2 -  Elastic Cloud Compute","text":""},{"location":"notes/aws-practitioner/compute/#amazon-machine-images-ami","title":"Amazon Machine Images - AMI","text":"<ul> <li>Large selection, own AMI, market place AMI</li> </ul>"},{"location":"notes/aws-practitioner/compute/#instance-types","title":"Instance types","text":"<ul> <li>An instance type simply defines the size of the instance based on a number of different parameters, these being ECUs. This defines:<ul> <li>vCPUs this is the number of virtual CPUs on the instance. </li> <li>Physical processor, this is the process speed used on the instance. </li> <li>Clock speed, it's clock speed in gigahertz. </li> <li>Memory, the amount of memory associated. </li> <li>Instance storage this is the capacity of the local instance store volumes available. </li> <li>EBS optimized available, this defines if the instance supports EBS optimized storage or not. </li> <li>Network performance, this shows the performance level of rate of data transfer. </li> <li>IPV6 support, this simply indicates if the instance type supports IPV6. </li> <li>Process architecture this shows the architected type of the processor. </li> <li>AES-NI, this stands for advanced encryption standard new instructions and it shows if the instance supports it for enhanced data protection. </li> <li>AVX this indicates if the instance supports AVX which is advanced vector extensions, which are primary used for applications focused on audio and video, scientific calculations and 3D modeling analysis. </li> <li>And finally Turbo which shows if the instance supports intel turbo boost and AMD turbo core technologies. </li> </ul> </li> <li>Instance Family<ul> <li>Micro instances - These are ideal for very low throughput use cases such as low traffic websites. </li> <li>General-purpose - balanced mix of CPU memory and storage making them ideal for small to medium databases, tests and development servers and back-end servers. </li> <li>Compute optimized - focus on compute. They have the highest performing processes for high-performance front end servers, web servers, high-performance science and engineering applications and video encoding and batch processing. </li> <li>GPU - GPU stands for graphics processing unit, optimized for graphic intensive applications. </li> <li>FPGA - field programmable gate arrays. To create application specific hardware accelerations when used with applications that use massively parallel processing power such as genomics and financial computing. </li> <li>Memory optimized -  primarily used for large-scale enterprise class in-memory applications, such as performing real time processing of unstructured data, Microsoft SharePoint. </li> <li>Storage optimized - optimized for enhanced storage. SSD backed instant storage for low latency and very high I/O, input/output performance, including very high IOPS which is input/output operations per second. And these are great for analytic workloads and no SQL databases. Data file systems and log processing applications. </li> </ul> </li> </ul>"},{"location":"notes/aws-practitioner/compute/#instance-purchasing-options","title":"Instance purchasing options","text":""},{"location":"notes/aws-practitioner/compute/#on-demand-instances","title":"on-demand instances","text":"<ul> <li>unpredictable</li> <li>testing and dev.</li> </ul>"},{"location":"notes/aws-practitioner/compute/#reserved-instances","title":"reserved instances","text":"<ul> <li>75% discount</li> <li>1- 3 years</li> <li>upfront payment, partial upfront, no up-front.</li> <li>long-term committed, steady workloads</li> </ul>"},{"location":"notes/aws-practitioner/compute/#scheduled-instances","title":"scheduled instances","text":"<ul> <li>fixed recurring schedule</li> <li>better rates than on-demand. charged even not used. </li> <li>scheduled but not continuously running workloads</li> </ul>"},{"location":"notes/aws-practitioner/compute/#spot-instances","title":"spot instances","text":"<ul> <li>bid higher than the current spot price</li> <li>2 minute warning if bid is not met</li> <li>low cost</li> <li>suddenly removed - batch jobs and background jobs. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#on-demand-capacity-reservations","title":"on-demand capacity reservations.","text":"<ul> <li>reserve capacity based on different attributes</li> <li>this can be used with reserved instances plan to save money</li> </ul>"},{"location":"notes/aws-practitioner/compute/#tenancy","title":"Tenancy","text":""},{"location":"notes/aws-practitioner/compute/#shared-tenancy","title":"shared tenancy","text":"<ul> <li>multiple customers on same host</li> <li>AWS manages security</li> </ul>"},{"location":"notes/aws-practitioner/compute/#dedicated-instances","title":"dedicated instances","text":"<ul> <li>hardware only be used by one aws account</li> <li>additional charge </li> <li>for security and compliance reasons. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#dedicated-hosts","title":"dedicated hosts","text":"<ul> <li>additional visibility into hardware</li> <li>placement of instances can be controlled</li> <li>socket and core visibility for licensing (per vm license)</li> <li>compliance, security and regulation requirements</li> </ul>"},{"location":"notes/aws-practitioner/compute/#user-data","title":"user data","text":"<ul> <li>runs onl once during first boot</li> <li>pull software updates and install during first boot</li> </ul>"},{"location":"notes/aws-practitioner/compute/#storage-options","title":"storage options","text":""},{"location":"notes/aws-practitioner/compute/#instance-store","title":"Instance Store","text":"<ul> <li>instance backed storage - all data is lost when stopped or terminated. Reboot retains data. Not detachable. </li> <li>Ephemeral storage / temporary. </li> <li>No additional cost and included as part of EC2. </li> <li>very high i/o speed. I3 - 3.3 million read IOPS/1.4 million write IOPS. </li> <li>Good for cache/buffer.</li> <li>Not all instances support instance type volumes. The sizes will increase with the size of the EC2 instance.</li> <li>Same security and policy as the EC2. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#ebs-storage","title":"EBS Storage","text":"<ul> <li>EBS storage - attached by network, persistent, EBS is auto replicated by AWS for resiliency, encryption and backup, size and performance can vary, Detachable and re-attachable. </li> <li>delete on termination. This is set to True by default for root volume. </li> <li></li> </ul>"},{"location":"notes/aws-practitioner/compute/#security","title":"security","text":"<ul> <li>Security Group - instance level firewall. Inbound and Outbound control. <ul> <li>Default</li> <li>All inbound traffic from same SG.</li> <li>All outbound traffic to any destination.</li> </ul> </li> <li>Key pair - Public and private key pair. Private key is your and public one remains with AWS. Used to decrypt password on windows or ssh for linux. Same key pair can be used across multiple instances. </li> <li>OS and its patches are your responsibility. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#status-checks","title":"Status Checks","text":"<ul> <li>System Status Check - Likely H/W issue, stop and start so that it launches somewhere else. Do not reboot as likely to remain on same server. </li> <li>Instance Status Check - EC2 instance itself, network issue, corrupted file system etc. Require user intervention. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#ec2-auto-scaling-group","title":"EC2 Auto Scaling Group","text":"<p>AWS Auto Scaling can handle scaling for Amazon ECS, DynamoDB and Amazon Aurora. </p>"},{"location":"notes/aws-practitioner/compute/#why","title":"Why","text":"<ul> <li>Automation</li> <li>Customer Satisfaction</li> <li>Cost Reduction</li> </ul>"},{"location":"notes/aws-practitioner/compute/#components","title":"Components","text":""},{"location":"notes/aws-practitioner/compute/#1-launch-template-configuration","title":"1. Launch Template Configuration","text":"<p>Two Options: Launch Template / Launch Configuration Include:  * AMI  * Instance Type  * Spot instance or not  * Public IP Address  * user data  * storage volume and configuration  * Security Groups</p>"},{"location":"notes/aws-practitioner/compute/#launch-template","title":"Launch Template","text":"<p>Newer version of Launch Configuration</p> <p>Create a Template:  * Template or Version  * Source Template  * AMI  * Instance Type  * Key/Pair  * VPC/Classic  * Security Groups  * Storage  * Tags  * Instance specific configurations (loads of them)  * user data</p>"},{"location":"notes/aws-practitioner/compute/#launch-configuration","title":"Launch Configuration","text":"<p>Create a Configuration:  * AMI  * Instance Type  * Spot or not  * IAM role  * Limited instance specific options  * user data  * Storage  * Security Groups</p>"},{"location":"notes/aws-practitioner/compute/#difference","title":"Difference","text":"<ul> <li>Single page for Launch Template</li> <li>Advanced options in Launch Template</li> </ul>"},{"location":"notes/aws-practitioner/compute/#2-auto-scaling-group","title":"2. Auto Scaling Group","text":""},{"location":"notes/aws-practitioner/compute/#group","title":"Group","text":"<ul> <li>Launch Template / Configuration choose. </li> <li>Adhere to Launch Template for fleet or combine purchase options</li> <li>Group Size</li> <li>Network --&gt; VPC and Subnets</li> <li>Association to Load Balancer</li> <li>Instance Protection (from scale in)</li> <li>service-linked role</li> <li>Keep to size or use policy to scale</li> </ul>"},{"location":"notes/aws-practitioner/compute/#policy","title":"Policy","text":"<ul> <li>Minimum and Maximum Size</li> <li>Policy Type</li> <li>Simple<ul> <li>Increase via Alarm which has metric of CPU Utilization add single instance</li> <li>Decrease via Alarm which has metric for CPU utilization remove single instance</li> <li>Cooldown - default 300s</li> <li>Health Check Grace - default 300s</li> </ul> </li> </ul> <p>Note</p> <p>Associate ASG directly to the Classic Load Balancer or to a Target Group from its configuration. ALB / NLB can then forward traffic to the target group and would automatically pickup all the EC2 instances in the ASG for load balancing. </p>"},{"location":"notes/aws-practitioner/compute/#ecs-elastic-container-service","title":"ECS - Elastic Container Service","text":"<ul> <li>No management software needed</li> <li>No monitoring software needed</li> <li>Clusters are region specific can't scale across regions. </li> <li>Clusters act as a resource pool and dynamically scalable</li> <li>Multiple instance type is possible</li> <li>Docker Demon and ECS agent on nodes of a cluster allowing ECS commands to be translated to Docker Commands. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#launch-options","title":"Launch Options","text":"<ul> <li>AWS Fargate</li> <li>CPU and Memory required</li> <li>Networking</li> <li>IAM policies</li> <li>Package application into containers</li> <li>EC2</li> <li>instance and instance patching</li> <li>more granularity</li> <li>security and compliance controls</li> </ul>"},{"location":"notes/aws-practitioner/compute/#aws-fargate","title":"AWS Fargate","text":"<p>Cluster management across EC2 instances.  AWS Fargate is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers. </p> <p>Docker is piece of software that allows you to automate the installation and distribution of applications inside Linux Containers. </p> <p>So what are containers? A Container holds everything that an application requires to enable it to run from within it's isolated container package. This may include system libraries, code, system tools, run time, etcetera. </p>"},{"location":"notes/aws-practitioner/compute/#ecr","title":"ECR","text":"<ul> <li>Fully managed by AWS for docker imgaes</li> </ul>"},{"location":"notes/aws-practitioner/compute/#registry","title":"Registry","text":"<ul> <li>Host and store docker images</li> <li>Your account w/r to images in registry</li> <li>IAM policies can control access to registry</li> <li>docker client needs to authorize to the default registry</li> </ul>"},{"location":"notes/aws-practitioner/compute/#authorization-token","title":"Authorization Token","text":"<ul> <li>AWS CLI get login for authorization. Outputs a docker login command</li> <li>Docker command to be used in docker client for login</li> <li>Token lasts for 12 hours. </li> <li><code>ecr:getAuthorizationToken</code> API call for an AWS user.</li> </ul>"},{"location":"notes/aws-practitioner/compute/#repository","title":"Repository","text":"<ul> <li>Grouping of images into repository for storage</li> <li>IAM and registry policies. IAM policies exist for RegistryFullAccess, RegistryPowerUser, RegistryReadOnly. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#repository-policy","title":"Repository Policy","text":"<ul> <li>Resource based policies and requires a principal</li> <li>Created within ERC itself and within the repositories. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#eks-elastic-container-service-for-kubernetes","title":"EKS - Elastic Container Service for Kubernetes","text":"<ul> <li>open source container orchestration tool. </li> <li>Can run Docker and Rocket containers</li> <li>AWS takes care of management infrastructure knowns as control plane. Ths is done across multi-AZ for resilience. </li> <li>User needs to maintain the work nodes. </li> <li>Control Plane does scheduling, tracking stake of all containers. It is made of:</li> <li>Number of APIs</li> <li>Kubelet Process</li> <li>Kubernetes Master</li> <li>Nodes - Worker machine. On-Demand EC2 instance and includes software to run containers via specific AMI which ensures Docker and Kublet in installed. Provisioned worker nodes connect to an EKS endpoint. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#working-with-eks","title":"Working with EKS","text":"<ol> <li>EKS Service Role - <code>AmazonEKSServicePolicy</code> and <code>AmazonEKSClusterPolicy</code>. Can be assigned to multiple clusters. </li> <li>EKS Cluster VPC - use Cloud Formation to run a specific template</li> <li>Install kubectl - kubernetes command line</li> <li>Install AWS-IAM-Authenticator - needed to authenticate to EKS Cluster</li> <li>Create EKS Cluster - use VPC details from step 2.</li> <li>Configure kubectl for EKS - <code>update-kubeconfig</code> on AWS CLI create a <code>kubeconfig</code> file for EKS cluster </li> <li>Provision and configure worker nodes - can be done via cloud formation using a template. </li> <li>Configure the worker node to join the EKS Cluster. </li> </ol>"},{"location":"notes/aws-practitioner/compute/#elastic-beanstalk","title":"Elastic BeanStalk","text":"<p>AWS Elastic Beanstalk is an AWS managed service that allows you to upload the code of your web application, along with the environment configurations, which will then allow Elastic Beanstalk to automatically provision and deploy the appropriate and necessary resources required within AWS to make the web application operational. </p> <p>These resources can include other AWS services and features, such as EC2, Auto Scaling, application health-monitoring, and Elastic Load Balancing, in addition to capacity provisioning. </p> <p>Elastic Beanstalk is able to operate with a variety of different platforms and programming languages, making it a very flexible service for your DevOps teams. There is no cost associated with Elastic Beanstalk, however, any resources that are created on your application's behalf, such as EC2 instances, you will be charged for as per the standard pricing policies at the time of deployment. </p>"},{"location":"notes/aws-practitioner/compute/#components_1","title":"Components","text":"<ul> <li>An application version is a very specific reference to a section of deployable code. The application version will point typically to S3, simple storage service to where the deployable code may reside. </li> <li>An environment refers to an application version that has been deployed on AWS resources.</li> <li>An environment configuration is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.</li> <li>The environment tier. This component reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do. You can deploy your application across one of two different environment tiers, either the web server tier or the worker tier. The web server environment is typically used for standard web applications that operate and serve requests over HTTP port 80. This tier will typically use service and features such as Route 53, Elastic Load Balancing, Auto Scaling, EC2, and Security Groups. The worker environment is slightly different and are used by applications that will have a back-end processing task that will interact with AWS SQS, the Simple Queue Service. This tier typically uses the following AWS resources in this environment, an SQS Queue, an IAM Service Role, Auto Scaling, and EC2. </li> <li>The configuration template. This is the template that provides the baseline for creating a new, unique environment configuration. </li> <li>The platform is a culmination of components in which you can build your application upon using Elastic Beanstalk</li> <li>Applications. Within Elastic Beanstalk, an application is a collection of different elements, such as environments, environment configurations, and application versions.</li> </ul>"},{"location":"notes/aws-practitioner/compute/#lambda","title":"Lambda","text":""},{"location":"notes/aws-practitioner/compute/#properties","title":"Properties","text":"<ul> <li>serverless</li> <li></li> </ul>"},{"location":"notes/aws-practitioner/compute/#how-does-it-work","title":"How does it work","text":"<ol> <li>Write code and upload to Lambda. Code can contain other libraries.</li> <li>Trigger the lambda to run the code. </li> <li>Computer power is ascertained and lambda code executes</li> <li>sub ms billing by aws. </li> </ol>"},{"location":"notes/aws-practitioner/compute/#components_2","title":"Components","text":"<ul> <li>Lambda application. Lambda function. The Lambda function is compiled of your own code that you want Lambda to invoke as per defined triggers. </li> <li>Event source. Event sources are AWS services that can be used to trigger your Lambda functions, or put another way, they produce the events that your Lambda function essentially responds to by invoking it. </li> <li>The Trigger is essentially an operation from an event source that causes the function to invoke. So essentially triggering that function. For example, an Amazon S3 put request could be used as a trigger. </li> <li>Downstream Resources. These are the resources that are required during the execution of your Lambda function. For example, your function might call upon accessing a specific SNS topic, or a particular SQS queue. So they are not used as the source of the trigger, but instead they are the resources to be used to execute the code within the function upon invocation.</li> <li>Log streams. In an effort to help you identify issues and troubleshoot issues with your Lambda function, you can add logging statements to help you identify if your code is operating as expected into a log stream. </li> </ul>"},{"location":"notes/aws-practitioner/compute/#aws-batch","title":"AWS Batch","text":"<p>Batch computing is primarily used in specialist use cases which require a vast amount of compute power across a cluster of compute resources to complete batch processing executing a series of jobs or tasks. </p>"},{"location":"notes/aws-practitioner/compute/#components_3","title":"Components","text":"<ol> <li>Jobs - A job is classed as a unit of work that is to be run by AWS Batch. For example, this can be a Linux executable file, an application within an ECS cluster or a shell script. </li> <li>Job definitions - These define specific parameters for the jobs themselves. They dictate how the job will run and with what configuration. Some examples of these may be how many vCPUs to use for the container, which data volume should be used, which IAM role should be used, allowing access for AWS Batch to communicate with other AWS services, and mount points. Can also define instance types and bid for spot instances.</li> <li>Job queues - Jobs that are scheduled are placed into a job queue until they run. It's also possible to have multiple queues with different priorities if needed.</li> <li>Job scheduling -  The Job Scheduler takes control of when a job should be run and from which Compute Environment.</li> <li>Compute Environments - These are the environments containing the compute resources to carry out the job. The environment can be defined as managed or unmanaged. Managed environments automates this using ECS. Unmanaged requires manual creation of ECS clusters. </li> </ol>"},{"location":"notes/aws-practitioner/compute/#amazon-lightsail","title":"Amazon Lightsail","text":"<p>Its a virtual private server (VPS) much like EC2. Simple, quick, easy to use and low cost point.  Host small website and blogs etc.  Multiple lightsail can be used and possible to connect to other infrastructure.  Lightsail sits outside the management console and configuration is a single page which is a simple version of EC2.  Pricing is per usage basis and is very simple to pick based on the requirements. </p>"},{"location":"notes/aws-practitioner/compute/#compute-savings-plans","title":"Compute Savings Plans","text":""},{"location":"notes/aws-practitioner/compute/#compute-savings-plans_1","title":"Compute Savings Plans","text":"<p>Compute Savings Plans provide the most flexibility and help to reduce your costs by up to 66%. These plans automatically apply to EC2 instance usage regardless of instance family, size, AZ, Region, OS or tenancy, and also apply to Fargate or Lambda usage. </p>"},{"location":"notes/aws-practitioner/compute/#ec2-instance-savings-plans","title":"EC2 Instance Savings Plans","text":"<p>EC2 Instance Savings Plans provide the lowest prices, offering savings up to 72% in exchange for commitment to usage of individual instance families in a Region (e.g. M5 usage in N. Virginia). This automatically reduces your cost on the selected instance family in that region regardless of AZ, size, OS or tenancy. EC2 Instance Savings Plans give you the flexibility to change your usage between instances within a family in that region. For example, you can move from c5.xlarge running Windows to c5.2xlarge running Linux and automatically benefit from the Savings Plan prices.</p>"},{"location":"notes/aws-practitioner/compute/#faq","title":"FAQ","text":""},{"location":"notes/aws-practitioner/compute/#outstanding-questions","title":"Outstanding Questions","text":""},{"location":"notes/aws-practitioner/database/","title":"Database","text":""},{"location":"notes/aws-practitioner/database/#database-types","title":"Database Types","text":"<ol> <li>Relational<ol> <li>MySQL</li> <li>MariaDB</li> <li>PostgreSQL</li> <li>Aurora</li> <li>Oracle</li> <li>SQL Server</li> </ol> </li> <li>Key-Value</li> <li>Document</li> <li>In-memory</li> <li>Graph</li> <li>Columnar</li> <li>Time Series</li> <li>Quantum Ledger</li> <li>Search </li> </ol>"},{"location":"notes/aws-practitioner/database/#types-of-workloads","title":"Types of workloads","text":"<ol> <li>OLTP - Online Transaction Processing. Predictable and structured work. </li> <li>OLAP - Online Analytics Processing. Retrospective, predictive, streaming. </li> </ol>"},{"location":"notes/aws-practitioner/database/#amazon-documentdb","title":"Amazon DocumentDB","text":""},{"location":"notes/aws-practitioner/database/#properties","title":"Properties","text":"<ul> <li>JSON Data as the value which is referred to as a document. </li> </ul>"},{"location":"notes/aws-practitioner/database/#amazon-keyspaces","title":"Amazon Keyspaces","text":""},{"location":"notes/aws-practitioner/database/#properties_1","title":"Properties","text":"<ul> <li>Columnar database</li> <li>Apache Casandra</li> </ul>"},{"location":"notes/aws-practitioner/database/#typical-usage","title":"Typical usage","text":"<ul> <li>Analytical process</li> <li>Big Data processing</li> <li>Data Warehouses</li> </ul>"},{"location":"notes/aws-practitioner/database/#elasticache","title":"Elasticache","text":"<ul> <li>In-memory DB used as cache</li> </ul>"},{"location":"notes/aws-practitioner/database/#memcached","title":"MemcacheD","text":""},{"location":"notes/aws-practitioner/database/#redis","title":"Redis","text":"<ul> <li>Persistence with transaction logs. </li> </ul>"},{"location":"notes/aws-practitioner/database/#neptune","title":"Neptune","text":""},{"location":"notes/aws-practitioner/database/#properties_2","title":"Properties","text":"<ul> <li>Graph Database</li> </ul>"},{"location":"notes/aws-practitioner/database/#amazon-timestream","title":"Amazon Timestream","text":""},{"location":"notes/aws-practitioner/database/#properties_3","title":"Properties","text":"<ul> <li>Time Series Database</li> <li>Data is gathered over time and stored with time as the key. </li> <li>Idea for Dev Ops applications which collect millions of data per second. </li> <li>IOT application data storage is also a good use case. </li> </ul>"},{"location":"notes/aws-practitioner/database/#amazon-quantum-ledger-database-qldb","title":"Amazon Quantum Ledger Database (QLDB)","text":""},{"location":"notes/aws-practitioner/database/#properties_4","title":"Properties","text":"<ul> <li>Cryptographic verification to confirm that data is immutable and remains unchanged. </li> <li>Quantum ledger uses block chain, hash of the record data and previous record data. </li> <li>Financial Transactional data that shouldn't change. </li> <li>Insurance history of claims</li> <li>Auditing is built in.</li> </ul>"},{"location":"notes/aws-practitioner/database/#amazon-elastic-search-service","title":"Amazon Elastic Search Service","text":"<ul> <li>Ingest data from multiple sources in raw format.</li> <li>Indexes the data and make it searchable.</li> <li>Each document is stored as json document.</li> <li>Elastic search uses reverse indexes - words against documents</li> </ul>"},{"location":"notes/aws-practitioner/database/#aws-rds-deployment","title":"AWS RDS Deployment","text":"<ul> <li>Instance Type and Instance Size</li> <li>Single-AZ vs Multi-AZ</li> <li>Failover 60-120s - DNS updated automatically.<ul> <li>Patching</li> <li>Host failure</li> <li>AZ Fails</li> <li>Reboot with failover</li> <li>Instance class on primary DB is modified. </li> </ul> </li> <li>EBS (scalable storage) used by MySQL, PostgreSQL, MariaDB, Oracle, SQL Server.</li> <li>SSD Storage (20GB - 64TB [SQLServer 16TB]), Provisioned (8000 IOPS - 80000 IOPS [SQLServer 40000] / 100GB - 64 TB [SQLServer 16TB])</li> <li>Autoscaling of storage can be enabled on configuration. &lt;10% storage for 5 mins and 6 hours since last scale up. </li> <li>Aurora - shared cluster storage.</li> <li>Grows automatically and doesn't need to be configured.</li> <li>Read Replica</li> <li>Read only traffic. </li> <li>Where multi-AZ config is enabled, read replica is created from secondary db.</li> <li>Auto Backup</li> <li>Backup to S3</li> <li>0-35 days of retention</li> <li>Manual backup can be done at any time and backups need to be deleted manually</li> <li>Encryption can be defined using KMS keys. </li> <li>Backtrack - Aurora allows backtrack up to 72 hours. Allows you to back in time. Aurora retains transactional log data for the duration of the backtrack. </li> </ul>"},{"location":"notes/aws-practitioner/database/#amazon-dynamo-db","title":"Amazon Dynamo DB","text":""},{"location":"notes/aws-practitioner/database/#properties_5","title":"Properties","text":"<ul> <li>NoSQL DB</li> <li>Key Value Store type. Key or indexes. </li> <li>Ultra high performance at any scale with single digit latency. </li> <li>Fully managed service</li> <li>No patches / No backup - all handled by AWS.</li> <li>Setup tables and provisioned throughput for each table. </li> <li>Pricing :  Provisioned Throughput + Size</li> <li>Creation requires DB name and Primary Key at its simplest form.</li> <li>Partition Key and Index can be used</li> <li>1 Query can use only 1 Index. Query needs to be explicit about what index to use.</li> <li></li> </ul>"},{"location":"notes/aws-practitioner/database/#secondary-index","title":"Secondary Index","text":"<ul> <li>Global - everywhere in the table</li> <li>Local - only within the single partition key. </li> </ul>"},{"location":"notes/aws-practitioner/database/#provisioned-capacity","title":"Provisioned Capacity","text":"<ul> <li>Read Capacity Units</li> <li>Write Capacity Units</li> <li>5 default.</li> <li>Each query can use more than one RCU/WCU</li> <li>Known load and query patterns.</li> <li>Auto-scaling can happen of the provisioned capacity based on utilization based on minimum and maximum capacity. </li> </ul>"},{"location":"notes/aws-practitioner/database/#on-demand","title":"On-Demand","text":"<ul> <li>Scaled on demand</li> <li>Not as cost-effective as standard.</li> </ul>"},{"location":"notes/aws-practitioner/database/#encryption","title":"Encryption","text":"<ul> <li>Default is on</li> <li>Customer managed or AWS managed Key can be selected. </li> </ul>"},{"location":"notes/aws-practitioner/database/#advantages","title":"Advantages","text":"<ul> <li>Fully managed - backup, redundancies</li> <li>Schemaless</li> <li>Auto HA - Across 3 AZ</li> <li>Fast ms and unlimited scalability.</li> <li>Constant performance. </li> </ul>"},{"location":"notes/aws-practitioner/database/#disadvantages","title":"Disadvantages","text":"<ul> <li>Eventual consistency due to replication</li> <li>Queries not flexible as SQL</li> <li>Max record size of 400KB</li> <li>20 global and 5 secondary index per table.</li> <li>Max number of tables per account - can be adjusted by AWS</li> <li>PovisionedThroughputException. </li> <li>Adjustments take a few mins. </li> </ul>"},{"location":"notes/aws-practitioner/database/#redshift","title":"Redshift","text":""},{"location":"notes/aws-practitioner/database/#properties-and-advantages","title":"Properties and advantages","text":"<ul> <li>Based on Postgre SQL</li> <li>Massively parallel processing</li> <li>Columnar data storage</li> <li>Result caching</li> </ul>"},{"location":"notes/aws-practitioner/database/#structure","title":"Structure","text":"<ul> <li>Cluster has a database and at least one compute node. More than one compute node has a leader node. </li> <li>Leader node is the gateway to the cluster and creates the execution plans for query. </li> <li>Compute Nodes can be of two types</li> <li>RA3 node types - High performance and scalable</li> <li>Dense node types - High performance with fixed local SSD</li> <li>Nodes can be between 1-32.</li> </ul>"},{"location":"notes/aws-practitioner/database/#communication","title":"Communication","text":"<ul> <li>JDBC</li> <li>ODBD</li> </ul>"},{"location":"notes/aws-practitioner/database/#metrics","title":"Metrics","text":"<ul> <li>Disk usage is available in CloudWatch</li> <li>CPU and Memory Utilization - Query/ Load performance data is only available from Redshift console and not CloudWatch. </li> </ul>"},{"location":"notes/aws-practitioner/database/#access-to-other-sources","title":"Access to other sources","text":"<ul> <li>Accessing data from S3 can be provided by IAM Roles to Redshift. </li> </ul>"},{"location":"notes/aws-practitioner/database/#backup","title":"Backup","text":"<ul> <li>Automated snapshots for backups</li> <li>Optionally - Backup cluster to a secondary region.</li> </ul>"},{"location":"notes/aws-practitioner/disaster-recovery/","title":"Disaster Recovery","text":"<ul> <li>back strategies need to be part of the disaster recovery planning</li> <li>How to decide RTO / RPO:</li> <li>Impact on business if app outage is extended?</li> <li>Cost of loss</li> <li>Dependencies</li> <li>Consumers</li> <li>Regulatory requirements</li> </ul>"},{"location":"notes/aws-practitioner/disaster-recovery/#rto","title":"RTO","text":"<ul> <li>Maximum amount of time a service is allowed to be unavailable.</li> </ul>"},{"location":"notes/aws-practitioner/disaster-recovery/#rpo","title":"RPO","text":"<ul> <li>Maximum amount of time for which data could be lost for a service.</li> </ul>"},{"location":"notes/aws-practitioner/disaster-recovery/#disaster-recovery-strategies","title":"Disaster Recovery Strategies","text":"Strategies RPO RTO Backup and Restore hours 24 hours or less Pilot Light mins hours Warm Standby seconds mins Multi-site close to zero close to zero"},{"location":"notes/aws-practitioner/disaster-recovery/#backup-restore","title":"Backup &amp; Restore","text":"<ul> <li>RTO is usually 24 hours and RPO is hours.</li> </ul> <p>Point in time recovery options  |Database | Storage|  |-------| -------|  |RDS Snapshot | EBS Snapshot|  |Aurora Snapshot | EFS Snapshot|  |DynamoBD Snapshot ||  |Redshift Snapshot ||  |DocumentDB Snapshot ||  |Neptune Snapshot ||</p> <p>### Pilot Light   * Introduction of data replication between DR Region   * Core infrastructure is in place in DR Region   * Ability to scale out faster   * RPO reduced as asynchronous data replication across regions.    * Servers are switched off and not running   * Cross Region data replications     * S3 cross-region replication - Automatically replicates objects from source to DR region     * RDF cross-region replicas - Replica DB in a separate region created from snapshots     * Aurora Global Database - low latency reads in multiple regions     * DyanmoDB global tables - multi-region deployment of table without need to crete replica     * DocumentDB global Clusters - single primary region and 5 replicas across regions     * Global Datastore for Amazon ElasticCache for Redis - cross-regional replica cluster and low latency reads</p>"},{"location":"notes/aws-practitioner/disaster-recovery/#warm-standby","title":"Warm Standby","text":"<ul> <li>Scaled down always running resources</li> <li>Can start serving request as soon as failure occurs</li> <li>Increased costs as opposed to Pilot Light</li> </ul>"},{"location":"notes/aws-practitioner/disaster-recovery/#multi-site-activeactive","title":"Multi-site Active/Active","text":"<ul> <li>Most complex</li> <li>Most Costly</li> <li>Lowest  RTO/RPO.</li> </ul>"},{"location":"notes/aws-practitioner/domains/","title":"Domains","text":""},{"location":"notes/aws-practitioner/domains/#domains-covered-in-the-exam","title":"Domains covered in the Exam","text":"<ol> <li>Security and Compliance</li> <li>Technology - CORE AWS services</li> <li>Billing and Pricing</li> <li>Cloud Concepts</li> </ol>"},{"location":"notes/aws-practitioner/domains/#cloud-concepts","title":"Cloud Concepts","text":"<p><code>Cloud Computing</code> :   Cloud computing is a remote virtual pool of on-demand shared resources offering to compute, storage, database and network services that can be rapidly deployed at scale.</p> <p><code>Resources Types</code> :   Compute, Storage, Database and Network.</p> <p><code>Cloud model types</code> :   Public Cloud, Private Cloud, Hybrid Cloud. These are differentiated based on Security, Data Location, Capital Expenditure, Operational Expenditure and Tenancy.</p> <p><code>Cloud Service Models</code> :   Infrastructure as a Service (IaaS), Software as a Service (SaaS) - Gmail, Platform as a Service (PaaS) -</p> <p><code>Dedicated Host</code> :   Choose instance placement, visibility of cores and sockets, meets licensing needs. </p> <p><code>Dedicated Instance</code> : More costly than shared, uses dedicated hardware without the visibility. </p> <p><code>High Availability</code> :   Replication across multiple geographical zones. </p>"},{"location":"notes/aws-practitioner/domains/#benefits-of-cloud","title":"Benefits of Cloud","text":"<ol> <li>On-demand</li> <li>Scalibility</li> <li>Economy of Scale</li> <li>Flexibility &amp; Scalibility</li> <li>Growth</li> <li>Utility based metering</li> </ol>"},{"location":"notes/aws-practitioner/domains/#use-cases-for-cloud-computing","title":"Use cases for Cloud Computing:","text":"<ul> <li>Traffic Bursting</li> <li>Backup and Disaster Recovery</li> <li>Test and Dev environments. </li> <li>Proof of Concepts. </li> <li>Big Data and Data Manipulation</li> </ul>"},{"location":"notes/aws-practitioner/domains/#aws-global-infrastructure","title":"AWS Global Infrastructure","text":""},{"location":"notes/aws-practitioner/domains/#availability-zone","title":"Availability Zone","text":"<p>At least one more AZ close to an AZ with high-speed, low-latency connectivity for data replication. In reality multiple-AZs are linked together. AZs will be in the same geographic area but will have independent power and networking to ensure that failure in one doesn't impact another. </p> <p>Multi-AZ deployment is best practice and maintains high availability of infrastructure in architecture design. </p> <p>AZ names us-east-1a may not designate the same AZ across accounts. This is done by AWS for better load balancing. </p> <p>61 AZs at present with another 12 planned. </p>"},{"location":"notes/aws-practitioner/domains/#region","title":"Region","text":"<p>Region is made up of a group of AZs within a certain geographical region. Every region is independent of one another will have at least three AZs. Regional deployments help with: 1. latency 2. regulation, laws and compliance  3. high availability and resiliency</p> <p>Most services are region specific and not all services are available in every region. </p> <p>IAM and CloudFront are not tied to a specific region. </p> <p>AWS GovCloud acts as an isolated region only available to the government and government-regulated industries that must meet strict guidelines. </p> <p>20 Regions at present with another 4 planned. </p>"},{"location":"notes/aws-practitioner/domains/#edge-location","title":"Edge Location","text":"<p>Places in major cities and highly populated areas and far outnumber the number of AZs. They are not used for traditional infrastructure.  They are used by Amazon CloudFront and Lambda@Edge for cache data and to reduce latency for end-user access by using edge location at CDN. </p>"},{"location":"notes/aws-practitioner/domains/#regional-edge-cache","title":"Regional Edge Cache","text":"<p>Type of Edge Location. These sit between Cloud Front Origin servers and Edge Locations and have larger cache-width.</p>"},{"location":"notes/aws-practitioner/domains/#aws-well-architected-framework-rspco","title":"AWS Well-Architected Framework (RSPCO)","text":""},{"location":"notes/aws-practitioner/domains/#operational-excellence","title":"Operational Excellence","text":"<p>aims at constant improvement and efficient managing of workloads, as well as gaining operational insights and continuous improvement of processes and procedures to support business value</p> <p>Principles:(1) Operations-as-Code (2) Frequent and reversible changes (3) Evolve procedures alongside the workload (4)Failure prevention (5)Learning from operational failures</p>"},{"location":"notes/aws-practitioner/domains/#reliability","title":"Reliability","text":"<p>focuses on the ability of a workload to perform correctly and as intended at the expected time. Including quickly recovery and prevention from failures</p> <p>Principles: (1)Automatic recovery (2) Test recovery procedures (3)Horizontal scaling for better availability (4) Stop guessing capacity (5) Manage changes in automation.</p>"},{"location":"notes/aws-practitioner/domains/#security","title":"Security","text":"<p>describes how to protect data, systems, and components using cloud technologies, how user rights and privileges are correctly managed, and how integrity and conformity of information is maintained.</p> <p>Principles: (1) Implementation of a strong identity foundation (2)Traceability (3) Apply security at all levels  (4) Security by automation  (5) Data protection (6) Access (7) Incident preparation</p>"},{"location":"notes/aws-practitioner/domains/#performance-efficiency","title":"Performance Efficiency","text":"<p>Efficient allocation and right-sizing of computing resources by the system requirements and interception of demand changes</p> <p>Principles: (1) Make use of advanced technologies (2) Global in minutes (3) Serverless architectures (4) Experiment more (5) Know the options - make the right choices</p>"},{"location":"notes/aws-practitioner/domains/#cost-optimization","title":"Cost Optimization","text":"<p>Understanding and controlling expenses, avoiding unnecessary costs, and analyzing spend in detail</p> <p>Principles: (1) Implement Cloud Financial Management (2) Adopt a consumption model (3) Measure overall efficiency (4) Stop spending money on data center operations (5)Analyze and attribute expenses</p>"},{"location":"notes/aws-practitioner/introduction/","title":"Introduction","text":"<p>This section will contain my study notes for the AWS Practitioner Exam. In particular these are my notes for preparing for CLF-C01.</p>"},{"location":"notes/aws-practitioner/introduction/#resource-list","title":"Resource List","text":"Resource Name Description Links Cloud Academy AWS Cloud Practitioner Certificate Preparation Learning Path Cloud Academy"},{"location":"notes/aws-practitioner/management/","title":"Management","text":""},{"location":"notes/aws-practitioner/management/#aws-cloudtrail","title":"AWS CloudTrail","text":"<ul> <li>Record and track all API call.</li> <li>Each API requests is captured as an event. </li> <li>The events are stored in log files within S3.</li> <li>Event Metadata:</li> <li>Identify of the caller</li> <li>Request timestamp</li> <li>Source IP Address</li> <li>New log files are created every 5 mins. </li> <li>Can be delivered to a CloudWatch Logs for metric monitoring and alerting via SNS.</li> <li>Global service and supported in all regions.</li> <li>Helps in tracking and compliance requirements. </li> </ul>"},{"location":"notes/aws-practitioner/management/#aws-config","title":"AWS Config","text":"<ul> <li>Record and capture resource changes in a file called a <code>configurationitem</code>.</li> <li>Acts as a resource inventory</li> <li>Discover compatible resources</li> <li>Stores configuration history</li> <li>Snapshot of current configuration </li> <li>Notification of changes via SN</li> <li>Integration with CloudTrail to check who/what was responsible for change</li> <li>Compliance check</li> <li>Security analysis. Custom rules such as are EBS volumes encrypted. </li> <li>Identify relationship between resources. </li> <li>Not all services are covered but major ones are. </li> <li>AWS config is region specific and needs to be configured for each region.</li> </ul>"},{"location":"notes/aws-practitioner/management/#aws-cloudwatch","title":"AWS CloudWatch","text":"<ul> <li>Global service</li> <li>Performance insights and can trigger automated responses</li> </ul>"},{"location":"notes/aws-practitioner/management/#components","title":"Components","text":"<ul> <li>Cloudwatch Dashboards - Visual widgets showing metrics and alarms. Multi-region. Build own views. Share dashboards to other users even outside your account.</li> <li>CloudWatch Metrics and Anomaly Detection - Different services will be relevant to different metrics. Metrics are by default calculated from data each 5mins. Extra fees can be paid to get data at 1 mins interval. Cloudwatch metrics are region specific. Anomaly Detection is done by Machine Learning to detect activity outside baseline parameters. </li> <li>CloudWatch Alarms - Tightly integrate with metrics. Set off at specific thresholds. Alarms can send messages to SNS topics. Alarms states - OK, Alarm, Insufficient Data.</li> <li>CloudWatch EventBridge - Enables event-driven real-time architecture. Established connection between your Apps and services. Wide range of targets are supported. Rules filter incoming events and sent to targets (same region). Events are captured via EventBus.</li> <li>CloudWatch Logs - Central repository of log monitoring. Unified CloudWatch Agent can capture additional metrics from EC2 instances. </li> <li>CloudWatch Insights - Insights allows for visual data to be created from cloudwatch logs. Types of insights:<ul> <li>CloudWatch Log Insights</li> <li>Container Insights</li> <li>Lambda Insights - Needs to be enabled per Lambda</li> </ul> </li> </ul>"},{"location":"notes/aws-practitioner/networking/","title":"Networking","text":""},{"location":"notes/aws-practitioner/networking/#ip-ranges","title":"IP Ranges","text":"<p>Reserved addresses with subnet 10.0.1.0/24</p> <ul> <li>10.0.1.0 - Network</li> <li>10.0.1.1 - AWS Routing</li> <li>10.0.1.2 - AWS DNS</li> <li>10.0.1.3 - AWS Future</li> <li>10.0.1.255 - Broadcast</li> </ul>"},{"location":"notes/aws-practitioner/networking/#vpc","title":"VPC","text":""},{"location":"notes/aws-practitioner/networking/#properties","title":"Properties","text":"<ul> <li>Isolated piece of the AWS Cloud only accessible by your AWS Account</li> <li>Define a name and a CIDR block from /16 to /28.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#limits","title":"Limits","text":"<ul> <li>5 VPC per region per account</li> <li>VPCs exist within a region.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#subnets","title":"Subnets","text":""},{"location":"notes/aws-practitioner/networking/#properties_1","title":"Properties","text":"<ul> <li>Reside inside a VPC and segment VPC.</li> <li>CIDR must reside within the CIDR of the VPC.</li> <li>Can be Public or Private.</li> <li>Each subnet has a route table. </li> <li>A route table can be attached to multiple subnets. </li> <li>Each subnet comes with a default route table with a local route that can't be deleted.</li> <li>Subnets have to reside within one AZs, can't span multiple AZs.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#public-subnet","title":"Public Subnet","text":"<ul> <li>Needs an entry in the route table pointing to the Internet Gateway.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#private-subnet","title":"Private Subnet","text":"<ul> <li>No route entry for the Internet Gateway.</li> <li>Needs a route entry pointing to a configured NAT Gateway in a public subnet to get outbound communications to the Internet. </li> </ul>"},{"location":"notes/aws-practitioner/networking/#nacl-network-access-control-list","title":"NACL - Network Access Control List","text":""},{"location":"notes/aws-practitioner/networking/#properties_2","title":"Properties","text":"<ul> <li>Provides security for subnets and controls traffic in and out of subnets.</li> <li>Default NACL is attached to each subnet created. Default NACL allows all traffic in and out. </li> <li>All NACLs have a default rule at the end of the list with Deny. Implicit Deny. </li> <li>There are Inbound NACLs and Outbound NACLs</li> <li>NACLs are stateless.</li> <li>Each subnet can have only one NACL.</li> <li>Same NACLs can be associated with multiple subnets.</li> <li>Evaluation stops at first match of a rule.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#structure","title":"Structure","text":"Inbound Outbound Rule Number Rule Number Type (HTTP) Type (HTTP) Protocol (TCP) Protocol (TCP) Port Range (80) Port Range (80) Source (0.0.0.0/-) Destination (0.0.0.0/-) Action (Allow/Deny) Action (Allow/Deny)"},{"location":"notes/aws-practitioner/networking/#security-groups","title":"Security Groups","text":""},{"location":"notes/aws-practitioner/networking/#property","title":"Property","text":"<ul> <li>Security Groups work at the instance level. </li> <li>Rules are an explicit Allow, they can't be deny. </li> <li>All Rules are evaluated before a decision is made.</li> <li>Implicit Deny if no rules match</li> <li>There are inbound and outbound security groups</li> <li>They are stateful</li> <li>Default - All inbound from same security group, All Outbound Traffic allowed to everywhere. </li> </ul>"},{"location":"notes/aws-practitioner/networking/#structure_1","title":"Structure","text":"Inbound Outbound Type (MySQL) Type Protocol (TCP) Protocol (TCP) Port Range (3306) Port Range Source 10.0.1.0/24 Source"},{"location":"notes/aws-practitioner/networking/#internet-gateway","title":"Internet Gateway","text":"<ul> <li>Attaches to VPC and allows connections to the Internet.</li> <li>Managed by AWS.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#nat-gateway","title":"NAT Gateway","text":"<ul> <li>Sits within the public subnet</li> <li>Needs an EIP</li> <li>Does not accept any inbound traffic from Internet</li> <li>Its managed by AWS. Has high availability setup by AWS.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#bastion-host","title":"Bastion Host","text":"<ul> <li>Resides on the public subnet, hardened, and allows incoming connections from Internet. </li> <li>Bastion hosts simply forwards incoming connections (ssh tunnel) to EC2s in the private subnet.</li> <li>Bastion host is essentially an EC2 instance which is keep secured and tightly controlled.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#vpn-connection","title":"VPN Connection","text":"<ul> <li>On-prem to AWS</li> <li>Customer Gateway (CGW) and VGW (Virtual Private Gateway)</li> <li>Encrypted traffic over the Internet. </li> <li>AWS Subnet Route tables will need routes to the On-prem resources. </li> <li>VPN tunnel can only be initiate from CGW.</li> <li>Idle activity causes connection drop. Monitoring to keep alive. </li> <li>Route Propagation can be turned on the route table and will allow routes to added to Route Tables.</li> <li>BGP on CGW enables dynamic routing and is recommended</li> <li>Security Groups also need to be configured for instances for the data connectivity. </li> </ul>"},{"location":"notes/aws-practitioner/networking/#direct-connect","title":"Direct Connect","text":"<ul> <li>Private infrastructure and does not traverse the public network</li> <li>Router on On-Prem, AWS Partner [Customer Side (Router) +  AWS Side (Router)], AWS Region.</li> <li>Direct Connect enables connection to AWS Defined Region. </li> <li>This enables access to public and private resources on AWS.</li> <li>The VPC would still need the Virtual Gateway (VGW) to enable on-prem traffic.</li> <li>Private Virtual Interface / Public Virtual Interface defines whether routes are to private subnet or public resources. Private Virtual Interface connects to VGW and Private Virtual Interface connects to Region. </li> <li>Dedicated network route from on-prem to Direct Connect Location. </li> <li>Its private connection and speeds of 1-10GBPs</li> </ul>"},{"location":"notes/aws-practitioner/networking/#vpc-peering","title":"VPC Peering","text":"<ul> <li>1-2-1 connection only. Not transitive. </li> <li>CIDR blocks can't overlap.</li> <li>Can peer VPC in different regions</li> <li>Requester requests a peering connection, Acceptor Accepts it. </li> <li>Route tables of both VPC needs to be updated to point to the peered connection. </li> </ul>"},{"location":"notes/aws-practitioner/networking/#transit-gateway","title":"Transit Gateway","text":"<ul> <li>A central hub to connect all VPCs for peering and VPN connections. </li> <li>Saves from paired connections and </li> <li>Connectivity can be achieved by individual connection directly to Transit Gateway.</li> <li>Centralized monitoring of network traffic as it goes through a central hub.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#route-53","title":"Route 53","text":""},{"location":"notes/aws-practitioner/networking/#properties_3","title":"Properties","text":"<ul> <li>Global network of DNS Authoritative servers.</li> <li>Hosted zone - how to route traffic for a particular domain</li> <li>Public zone - how is traffic routed on the public internet</li> <li>Private zone - how is traffic routed inside AWS.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#record-types","title":"Record Types","text":"Record Type Description A Route traffic to a resource using IP4 AAAA Route traffic to a resource using IP6 CAA Certificate authority for domain and subdomains CNAME Used as an Alias of one name to another. Does not work for APEX. MX Mail Server NAPTR Name Authority Pointer NS Named server for hosted zone PTR Map IP to domain name <p>Alias is a route 53 specific extension to DNS. Routes traffic to:</p> <ul> <li>S3 buckets</li> <li>ELBs</li> <li>Elastic Beanstalk</li> <li>CloudFront Distributions</li> <li>VPC Interface Endpoints</li> </ul>"},{"location":"notes/aws-practitioner/networking/#routing-policies","title":"Routing Policies","text":"Routing Policy Description Simple Default, no health check, single resource. Failover Route to different policy based on health checks, active passive failover Geo-Location Geographic location/origin of traffic Geo Proximity Based on both users and resource location. Bias allows to adjust scope of resource and controls routing of traffic. Latency Routing based on lowest latency Multi-value Returns up to 8 records at once Weighted Routing Policy Distributes traffic according to weights"},{"location":"notes/aws-practitioner/networking/#cloudfront","title":"CloudFront","text":"<ul> <li>AWS CDN</li> <li>Uses Edge locations around the world to delivery best performance through cached data close to the user. </li> <li>Distributions Types:</li> <li>Web - static / dynamic content, media using http/https, submit data via web forms, live streaming in real-time</li> <li>RTMP Distribution - Adobe Flash Media distribution. User can start watching video before file downloads. Source can only be S3 bucket not EC2.</li> <li>Origin can be EC2 server or S3 Bucket. If S3 is static hosting website, must put point to the static site endpoint.</li> <li>If S3 is used - Origin Access Identity (OAI) can be used to restrict access stopping direct access of S3 without cloudfront. </li> <li>Caching behavior options can be controlled via methods and policies</li> <li>Can select Regions where edge location distributions will be places/used.</li> <li>WAF access control list can be placed on distribution</li> <li>Implementation of additional encryption can be setup with SSL Certificates</li> </ul>"},{"location":"notes/aws-practitioner/networking/#aws-global-accelerator","title":"AWS Global Accelerator","text":"<ul> <li>Get UDP and TCP traffic from end users to AWS endpoints via AWS infrastructure rather than public network. Optimized path for lowest latency and avoids unhealthy resources</li> <li>Uses 2 fixed IP addresses. Can use own IP or from AWS pool. These are mapped to multiple Global Accelerator endpoints. </li> <li>Global Accelerator can be multi-region and can forward traffic to:</li> <li>ELB</li> <li>EC2</li> <li>EIP</li> <li>Creation has 4 steps:</li> <li>Create accelerator, give it name and select 2 IP Address.</li> <li>Listener - TCP/UDP based and ports</li> <li>Associate listener with Endpoint Group with multiple endpoints which are for specific Region. Traffic dials can be used to weight traffic routing. Helps with Blue/Green deployments. Setup of health checks can be done at this stage</li> <li>Register and associate endpoints for Applications. Each endpoint can also have a weighted traffic routing within the endpoint group.</li> <li>Client affinity is supported for continued connections.</li> </ul>"},{"location":"notes/aws-practitioner/networking/#elb-elastic-load-balancer","title":"ELB - Elastic Load Balancer","text":"<p>Targets could be a fleet of:</p> <ul> <li>EC2 instances, </li> <li>Lambda functions, </li> <li>a range of IP addresses, </li> <li>or even Containers.</li> </ul> <p>Managed by AWS and is elastic. </p>"},{"location":"notes/aws-practitioner/networking/#comparison","title":"Comparison","text":"<p>AWS Comparison Table</p>"},{"location":"notes/aws-practitioner/networking/#components","title":"Components","text":"<ul> <li>Listener - The listener defines how your inbound connections are routed to your target groups based on ports and protocols set as conditions.</li> <li>Target Groups - A target group is simply a group of resources that you want your ELB to route requests to, for example a fleet of EC2 instances. </li> <li>Rules - Rules are associated to each listener that you have configured within your ELB, and they help to define how an incoming request gets routed to which target group. </li> </ul>"},{"location":"notes/aws-practitioner/networking/#elb-nodes","title":"ELB Nodes","text":"<p>An ELB node will need to be placed to any Availability Zones for which you want to route traffic to. Without the Availability Zone associated, the ELB will not be able to route traffic to any targets within that Availability Zone even if they are defined within the target group. This is because the nodes are used by the ELB to distribute traffic to your target groups. </p>"},{"location":"notes/aws-practitioner/networking/#internal-or-internet-facing-elbs","title":"Internal or Internet-facing ELBs.","text":"<ul> <li>Internet-facing ELBs - public DNS name, public IP address, internal IP address </li> <li>Internal - Internal IP address. </li> </ul> <p>Communication to Target Group is only done via Internal IP. </p>"},{"location":"notes/aws-practitioner/networking/#cross-zone-load-balancing","title":"Cross-Zone load balancing","text":"<ul> <li>Not set - distributes to nodes</li> <li>Set - distributes to targets evenly. </li> <li>Default - ALB on,  NLB off</li> </ul>"},{"location":"notes/aws-practitioner/networking/#alb","title":"ALB","text":"<ul> <li>HTTP/HTTPs protocol.</li> <li>Request level</li> <li>Advanced Routing</li> <li>TLS Termination</li> <li>Route to particular ports / targeted </li> <li>Visibility features for application architecture.</li> <li>cross-zone is always on. </li> </ul>"},{"location":"notes/aws-practitioner/networking/#configuration","title":"Configuration","text":""},{"location":"notes/aws-practitioner/networking/#target-group-configuration","title":"Target Group Configuration","text":"<p>Define a Target Group, consisting of:</p> <ul> <li>Type<ul> <li>Instance</li> <li>IP</li> <li>Lambda</li> </ul> </li> <li>Protocol, Port and VPC</li> <li>Health check<ul> <li>Protocol</li> <li>Path</li> <li>Healthy/unhealthy thresholds</li> <li>Interval</li> <li>Success codes</li> </ul> </li> <li>Register Targets in the group</li> </ul>"},{"location":"notes/aws-practitioner/networking/#load-balancer-configuration","title":"Load Balancer Configuration","text":"<ul> <li>Public / Private</li> <li>Listeners</li> <li>Rules can be added under listeners</li> <li>Multiple Rules can exist.</li> <li>Select AZs for node deployment</li> <li>Security Group for the Load Balancer</li> <li>Configure Routing to Target Group</li> </ul>"},{"location":"notes/aws-practitioner/networking/#nlb","title":"NLB","text":"<ul> <li>connection level</li> <li>ultra-high performance - millions of requests per second. </li> <li>TCP/UDP protocol. Layer 4.</li> <li>Listeners - TCP/TLS/UDP.</li> <li>cross-zone can be turned on or off, default off. </li> </ul> <p>If your application requires a Static IP then NLB is the only option. </p> <p>Once a connection is established, the connection remains open for the duration of the request. </p>"},{"location":"notes/aws-practitioner/networking/#configuration_1","title":"Configuration","text":"<ul> <li>Public / Private</li> <li>Listeners</li> <li>Rules can be added under listeners</li> <li>Multiple Rules can exist.</li> <li>Select AZs for node deployment</li> <li>Security Group for the Load Balancer</li> <li>Configure Routing to Target Group</li> </ul>"},{"location":"notes/aws-practitioner/networking/#classic-load-balancer","title":"Classic Load Balancer","text":"<ul> <li>Classic environment</li> <li>Operates at both request and connection level. </li> </ul> <p>Note</p> <p>For target we select EC2 instances directly and not target groups.</p>"},{"location":"notes/aws-practitioner/networking/#health-check","title":"Health Check","text":"<p>Health checks. The ELB associates a health check that is performed against the resources defined within the target group. These health checks allow the ELB to contact each target using a specific protocol to receive a response. If no response is received within a set of thresholds, then the ELB will mark the target as unhealthy and stop sending traffic to that target.</p>"},{"location":"notes/aws-practitioner/networking/#server-certificate-ssltls","title":"Server Certificate (SSL/TLS)","text":"<p>HTTPS on ALB requires additional configuration. The server certificate used by ALB is X.509. Certificate can be issued by ACM (AWS Certificate Manager). ACM doesn't work in every region and there you need your own certificates in IAM. </p> <p>Certificate Selection:  1. Choose certificate from ACM  2. Upload to ACM  3. Choose from IAM  4. Upload to IAM</p>"},{"location":"notes/aws-practitioner/networking/#outstanding-question","title":"Outstanding Question","text":""},{"location":"notes/aws-practitioner/other-services/","title":"Other Services","text":""},{"location":"notes/aws-practitioner/other-services/#aws-datasync","title":"AWS DataSync","text":""},{"location":"notes/aws-practitioner/other-services/#between-non-aws-storage","title":"Between Non-AWS Storage","text":"<ul> <li>Snow devices to S3</li> <li>On-prem data center to S3</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#between-aws-storage","title":"Between AWS Storage","text":"<ul> <li>EBS to S3</li> <li>Windows FXs to S3</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-opshub","title":"AWS OpsHub","text":"<ul> <li>Comes as a software and not on the AWS Console. </li> <li>Manages the SNOW devices for data transfer</li> <li>Configure fleets of cluster devices</li> <li>Dashboard and Metrics to the snow devices.</li> <li>Integrates with AWS Systems Manager for automation of different tasks. </li> <li>GUI based as opposed to previous way of managing via AWS CLI. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-lambda","title":"AWS Lambda","text":"<ul> <li>Serverless</li> <li>Runs code without having to define servers</li> <li>Event-drive triggers, e.g. S3 notifications</li> <li>Time limits of 15 mins, and 10GB memory.</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-fargate","title":"AWS Fargate","text":"<ul> <li>Serverless containers on ECS, no limits like Lambda. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#amazon-eventbridge","title":"Amazon EventBridge","text":"<ul> <li>Serverless event bus</li> <li>Takes input events from multiple sources and can act as the event bus from there. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-step-function","title":"AWS Step Function","text":"<ul> <li>serverless state machine</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#amazon-api-gateway","title":"Amazon API Gateway","text":"<ul> <li>Building, monitoring, documenting and managing serverless API</li> <li>100s of Thousands of requests per second and can throttle if necessary</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-appsync","title":"AWS AppSync","text":"<ul> <li>Manage and Sync data across multiple data across mobile devices and users</li> <li>build real-time multi-user collaborative tools, between different apps. </li> <li>AWS AppSync enables a single GraphQL API to access data from multiple sources</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#amazon-rds-proxy","title":"Amazon RDS Proxy","text":"<ul> <li>Serverless, highly available proxy for Amazon RDS.</li> <li>More scalable than standard direct to RDS implementation</li> <li>Pools established database connections for enabling connections at scale</li> <li>Can refuse connections that can't be serviced. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#amazon-cognito","title":"Amazon Cognito","text":"<ul> <li>Used for managing federated users access to roles. Specifically recommended for Mobile applications. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-trusted-advisor-cpsfs","title":"AWS Trusted Advisor (CPSFS)","text":"<p>Makes recommendations to improve infrastructure use across environment based on known best practices.</p> <p>Make recommendations in the following areas:   * Cost Optimization - is there unused resources.   * Performance - where can we take advantage of provisioned capabilities   * Security - weakness/vulnerabilities   * Fault Tolerance - Multi-AZ   * Service Limits - 80% limits are highlighted</p> Basic Developer Business Enterprise Cost Optimization Performance Security 6 core checks 6 core checks Fault Tolerance Service Limits Track recent changes API CloudWatch Integration <p>Core Security Checks:  * S3 bucket permissions  * Security Groups - specific ports unrestricted  * EBS public snapshot  * RDS public snapshot  * IAM use  * MFA on Root account.</p> <p>All Plans get:  * Trusted Advisor Notification (opt in/out)  * Exclude items  * Action Links  * Access Management  * Refresh - 24 hourly update, manual refresh at 5 mins interval.</p>"},{"location":"notes/aws-practitioner/other-services/#aws-organizations","title":"AWS Organizations","text":"<ul> <li>Centrally manage multiple AWS accounts. Helps with security &amp; compliance and account management. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#components","title":"Components","text":""},{"location":"notes/aws-practitioner/other-services/#organization","title":"Organization","text":"<ul> <li>A hierarchical structure for Accounts. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#root","title":"Root","text":"<ul> <li>Top level of organization</li> <li>Single root at an organization</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#organizational-unit","title":"Organizational Unit","text":"<ul> <li>Groups of Accounts, hierarchical structure</li> <li>Can be attached to root or another OU</li> <li>Can be 5 level deeps</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#account","title":"Account","text":"<ul> <li>AWS Accounts an be controlled from a Master Account.</li> <li>Master Account can be used for consolidated billing from all OUs and Accounts. </li> <li>Master Account should be kept to a minimum as it can create additional accounts, invite accounts, remove accounts and apply SCP.</li> <li>Two types:</li> <li>Enable Consolidated Billing</li> <li>Enable all features - needed for SCP. </li> <li>Master Account and Resource-based linked policies are not impacted by SCPs</li> <li>Root Account is still able to perform a lot of functionalities even after an attached SCP. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#service-control-policies","title":"Service Control Policies","text":"<ul> <li>Sets the permission boundary</li> <li>Can be applied to Root, OU or Account. </li> <li>Once set it applies to all children of the leaf where set</li> <li>Does not grant policies</li> <li>AWS organizations with 'Enable All Settings' are required and needs to be setup from root account</li> <li>By default the root account gets a SCP of <code>FullAWSAccess</code>. This is why everything works by default.</li> <li>SCPs don't affect resource-base policies then affect the principals. </li> <li>SCPs affect all users, role in addition to root. Root still able to change password, MFA settings, root access keys and x.509 certificates for root. </li> <li>Organization removal - removes all SCPs. Re-enabling goes back to default. </li> <li>SCPs do not affect - Master Account, service-linked roles and CloudFront Keys.</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-waf-web-application-firewall","title":"AWS WAF (Web Application Firewall)","text":"<ul> <li>Helps with mitigation of attack patterns: OWASP Top 10 Attack Patterns</li> <li>Filtering request on source IP address or country of origin</li> <li>Reduces the risk by block traffic at the perimeter.</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#integrations","title":"Integrations","text":"<ul> <li>Amazon API Gateway</li> <li>CloudFront Distributions</li> <li>Application Load Balancer</li> <li>AWS AppSync GraphQL API</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#traffic-types","title":"Traffic Types","text":"<ul> <li>HTTP </li> <li>HTTPS</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#components_1","title":"Components","text":""},{"location":"notes/aws-practitioner/other-services/#web-acls","title":"Web ACLs","text":"<ul> <li>Main building block of WAF service</li> <li>Associated with the integrated services to allow of block traffic</li> <li>Contains rules to allow/block traffic</li> <li>Default rule also exists to allow/block</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#rules","title":"Rules","text":"<ul> <li>Statement (criteria)</li> <li>Action</li> <li>Allow - forwarded</li> <li>Block - blocked and informs requester of block</li> <li>Count - counts traffic match</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#rule-groups","title":"Rule Groups","text":"<ul> <li>A collection of rules that can be applied to Web ACLs</li> <li>WAF comes with some AWS collections or can be purchased from AWS Marketplace</li> <li>You can do your own</li> <li>Rule Groups have weights and this limits the number of rules that can be put on WAF to ensure performance. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-firewall-manager","title":"AWS Firewall Manager","text":"<ul> <li>Simplify the management of security protection across multiple-AWS Accounts. </li> <li>AWS Firewall manager can apply the security / firewall policies across multiple Accounts and automatically new resources. </li> <li>Simplifies security management</li> <li>Integrates with AWS organization and is required for using AWS Firewall manager</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#integration","title":"Integration","text":"<ul> <li>AWS WAF</li> <li>AWS Shield Advanced</li> <li>AWS Network Firewall</li> <li>VPC Security Groups</li> <li>Amazon Route53 Resolver DNS Firewall</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#steps-for-configuraiton","title":"Steps for configuraiton","text":"<ol> <li>AWS Firewall Manager Administrator Account - define rules</li> <li>Account has to be part of an organization</li> <li>AWS Firewall manager as the Firewall Manager account. </li> <li>Enable AWS Config for all accounts, all regions for firewall manager to work</li> <li>Enable Sharing with AWS Organizations in Resource Access Manager</li> <li>Enable regions within AWS Management console where firewall manager is no enabled by default.</li> </ol>"},{"location":"notes/aws-practitioner/other-services/#aws-shield","title":"AWS Shield","text":"<ul> <li>Protect against DDOS attacks:</li> <li>SYN Flood</li> <li>DNS Query flood</li> <li>HTTP Flood/Cache-busting</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#levels-of-featurs","title":"Levels of Featurs","text":"<ul> <li>Standard - free for all AWS Account. Protect against Level 3 and Level 4 attacks. </li> <li>Advanced - Web Apps on EC2, CloudFronts, ELBs and Route53. 24x7 DDOS protection team. Level 3,4,7 protection. Cost protection and get AWS WAF included. $3K / month current cost.</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#amazon-inspector","title":"Amazon Inspector","text":"<ul> <li>Analyze EC2 instance for security issues</li> <li>Rules for security standards and vulnerability definitions</li> <li>Install agent on the EC2</li> <li>Gives a prioritized list of findings </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#amazon-guard-duty","title":"Amazon Guard Duty","text":"<ul> <li>Intelligent Threat Identification using AI</li> <li>Analyze CloudTrail logs, VPC Flow logs and DNS query logs for review</li> <li>Sample Findings are provided for each type of problem.</li> <li>Critical, Medium and Low severity findings. </li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-security-hub","title":"AWS Security Hub","text":"<p>???</p>"},{"location":"notes/aws-practitioner/other-services/#amazon-macie","title":"Amazon Macie","text":"<ul> <li>Machine learning to analyze objects in S3</li> <li>Identifies critical private data - PII</li> <li>Automatically keeps track of changes to data to only check new objects</li> <li>One time or scheduled run possible</li> <li>Reports the findings as Medium or High and the type of findings.</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-resilience-hub","title":"AWS Resilience Hub","text":"<ul> <li>Information about how resilient is your design.</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-backup","title":"AWS Backup","text":"<ul> <li>Can be used to manage backup across multiple AWS services</li> <li>Central managed service</li> <li>Supports multiple regions</li> <li>Central backup hub</li> <li>logging and monitoring</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#policies","title":"Policies","text":"<ul> <li>schedule</li> <li>window</li> <li>lifecycle rules</li> <li>backup vault </li> <li>regional copies </li> <li>tags</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#integrates","title":"Integrates","text":"<ul> <li>EFS needs to use AWS Backup</li> <li>EBS</li> <li>EC2</li> <li>RDS</li> <li>Aurora</li> <li>DynamoDB</li> <li>DFS</li> <li>Storage Gateway Volumes</li> <li>FSx (Windows and Lustre)</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#data-lifecycle-manager","title":"Data Lifecycle Manager","text":"<ul> <li>EBS Backup across cross-region</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#aws-well-architected-tool","title":"AWS Well-Architected Tool","text":"<ul> <li>Based on best practices from AWS</li> <li>based on AWS Well-Architected Framework</li> </ul>"},{"location":"notes/aws-practitioner/other-services/#monitoring","title":"Monitoring","text":""},{"location":"notes/aws-practitioner/other-services/#cloudwatch","title":"CloudWatch","text":"<ul> <li>metrics for cluster size scaling up and down for ECS</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/","title":"Decoupling and Queuing","text":""},{"location":"notes/aws-practitioner/queuing-service/#properties","title":"Properties","text":"<ul> <li>Loose coupling</li> <li>Asynchronous connectivity between sub-systems.</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#amazon-sqs","title":"Amazon SQS","text":"<ul> <li>Producers put messages to the SQS Queue</li> <li>Consumer is polling the queue for pick up and processing</li> <li>Visibility Timeout is set when picked up by consumer</li> <li>Delete message from Consumer deletes the message or visibility timeout expiry causes message to become visible on the queue.</li> <li>Visibility Timeout - min 0s, Default 30s and maximum is 12 hours. </li> <li>Visibility timeout can be setup for entire queue or individual messages. </li> <li>Step functions should be used if more than 12 hours delay is needed. </li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#standard-queue","title":"Standard Queue","text":"<ul> <li>Standard Queue - At least delivery guaranteed. </li> <li>120K in-flight messages in a standard queue. Short Polling will return error if exceeded, Long polling won't return any errors.</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#fifo-queues","title":"FIFO Queues","text":"<ul> <li>First in First Out, implements exactly once delivery. </li> <li>Lower performance than Standard Queue. </li> <li>20K in-flight messages. </li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#dead-letter-queue","title":"Dead Letter Queue","text":"<ul> <li>Messages are Delivered here after <code>MaxReceivedCount</code> reaches the limits set for a Queue. </li> <li>Messages can be put back into the original queue once the problem has been resolved but may break FIFO order in doing so. </li> <li>SNS is commonly used to dispatch notification when messages arrive in dead letter queue for examination. </li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#delay-queue","title":"Delay Queue","text":"<ul> <li>Delay queue - the message is not initially available until the delay timeout is over. Can range min 0s and max is 15mins</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#consumer-polling","title":"Consumer polling","text":""},{"location":"notes/aws-practitioner/queuing-service/#short-polling","title":"Short polling","text":"<ul> <li>Default for queues</li> <li>Consumers use ReceiveMessage request and SQS responds even when no messages are found</li> <li>This can happen when either the consumer sets <code>ReceiveMessage</code> or queue parameter <code>ReceiveMessageWaitTime</code> is set to 0.</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#long-polling","title":"Long polling","text":"<ul> <li><code>ReceiveMessage</code> or <code>ReceiveMessageWaitTime</code> is set to larger than 0 and less than 20 seconds. </li> <li>SQS waits for at least one message or the max number <code>MaxNumberOfMessages</code> specified. </li> <li>Empty response happens when the Wait time expires. </li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#messasge-specifics","title":"Messasge Specifics","text":"<ul> <li>Minimum size is 1 byte / 1 character</li> <li>Maximum is 256 KB (default)</li> <li>SQS Extended Client Library enables messages to be 2GB and leverages S3.</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#sns-simple-notification-service","title":"SNS - Simple Notification Service","text":""},{"location":"notes/aws-practitioner/queuing-service/#property","title":"Property","text":"<ul> <li>1:M Messages distribution model</li> <li>A single message is published to a Topic which is then pushed out to subscribers. </li> <li>A Topic doesn't store messages, once published the message are pushed out to subscribers. </li> <li>A single topic can have 12M subscriptions</li> <li>100K Topics are allowed and </li> <li>US-EAST1 allows a rate of 30K/seconds which varies by regions. </li> <li>Max Message is 256KB and breaks into chunks of 64KB and deals with them as individual requests.</li> <li>Extended Library can increase the message size to 2GB using S3.</li> <li>Message Filtering allows subscribers to filter messages using SNSMessagePolicy via JSON specifications. Removes the needs to have multiple Topics for each Subscriber but enables filtering on the single topic from Producer.</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#message-retry","title":"Message Retry","text":"<ul> <li>Controlled using SNS Delivery Policy</li> <li>Retry Patterns:</li> <li>Linear</li> <li>Geometric</li> <li>Exponential back-off</li> <li>maximum and Minimum tries</li> <li>Messages can be sent to Dead Letter Queue which can be delivered.</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#notification-services","title":"Notification Services:","text":"<ul> <li>Apple Push</li> <li>Windows Push</li> <li>Amazon Messages</li> <li>HTTPS endpoints</li> <li>Email</li> <li>SMS </li> <li>MFA</li> <li>OTP</li> </ul>"},{"location":"notes/aws-practitioner/queuing-service/#amazon-mq-service","title":"Amazon MQ Service","text":"<ul> <li>Enables migration of existing on-prem queues to Cloud</li> <li>Amazon MQ is AWS managed message broker service for Apache ActiveMQ, and its compliant with:</li> <li>JMS, </li> <li>NMS, </li> <li>MQTT and </li> <li>WebSockets.</li> <li>Amazon MQ also provides message encryption in transit using SSL and at rest using AES 256 encryption.</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/","title":"Security and Compliance","text":""},{"location":"notes/aws-practitioner/security-compliance/#aws-artifact","title":"AWS Artifact","text":"<ul> <li>Self-service AWS Security Reports</li> <li>View and manage contacts between you and AWS for Account or Organization. </li> <li>Reports</li> <li>Audit Compliance Artifacts/Reports, can be shared with IAM users (Auditors).</li> <li>Audit artifacts validate AWS portion of the service not user</li> <li>Agreements</li> <li>Can be applied to all Accounts in an Organization</li> <li>NDA needs to be signed before the agreement can be viewed</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#iam","title":"IAM","text":""},{"location":"notes/aws-practitioner/security-compliance/#components","title":"Components","text":"<ul> <li>Identity - username - Which user/who is it?</li> <li>Authentication - Password - Prove that you are the users.</li> <li>Authorization - Access to services and features. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#authentication","title":"Authentication","text":"<ul> <li>username/password</li> <li>MFA</li> <li>Federated Access (no AWS IAM user credential needed)</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#features","title":"Features","text":"<ul> <li>Setup password complexity requirements. Enforce password policies. </li> <li>MFA required (should be setup for elevated users at least)</li> <li>IAM user sign-in links (customized)</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#access-management-resources","title":"Access Management - Resources","text":""},{"location":"notes/aws-practitioner/security-compliance/#users","title":"Users","text":"<ul> <li>Can be people </li> <li>Can be Applications</li> <li>Has a specific ARN</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#user-groups","title":"User Groups","text":"<ul> <li>Groups users</li> <li>Associated to IAM policies </li> <li>Used to often related to a job-role.</li> <li>Users inherit the permissions of the group..</li> <li>Default 300 Groups</li> <li>An user can be attached to 10 Groups and 10 policies can be attached to a group.</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#iam-policies","title":"IAM policies","text":"<ul> <li>AWS Managed Policies  - written by AWS and stored in library</li> <li>Customer managed Policies - defined by customer and stored in library</li> <li>in-line policies - policies attached directly to a user, group or role. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#roles","title":"Roles","text":"<ul> <li>Identity with associated permissions</li> <li>Assumed by identity or service that need to acquire temporary permissions. </li> <li>No passwords, assumed if correctly permissioned.</li> <li>Roles can be assumed by - trusted users, AWS services and applications. This is done using a trust relationship.</li> <li>Policies are associated with Roles just like other identities</li> <li>Roles an be assumed cross-account by users, federated users as well. </li> <li>Temporary ability to get permissions or capabilities, by using Roles.</li> <li>When users assume a role, users existing permissions are replaced and only has the permissions attached to the role. </li> <li>For a Role, trusted entity has to include a user, as well as, a policy needs to be associated with the user to be able to assume the role. So this has to be both way, trusted by the role policy and policy for the user to assume the role. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#aws-service-roles","title":"AWS Service Roles","text":"<ul> <li>Allows AWS services to access other AWS resource on your own account. </li> <li>For example - EC2 gets access to AWS Resources. EC2 needs to access to S3. Removes the need to store credentials locally.</li> <li>Service-linked roles Can only be used by specific AWS services to interact with other AWS services. Created at the time of first use and uses AWS managed polices. Example - <code>AWSServiceRoleForAmazonSSm</code>, <code>AWSServiceRoleForCloudTrail</code> and <code>AWSServiceRoleForCloudWatchEvents</code>. It will have a fixed service principal that can't be edited. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#federated-access-to-roles","title":"Federated Access to Roles","text":"<ul> <li>Two options:</li> <li>Web Identity - Large base of external users using Google, Facebook etc. </li> <li>SAML2.0 federation - Employees of the firm using existing directory services</li> <li>Identity Provider Authenticates the user and Service Provider enables access to services once authentication. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#identify-providers","title":"Identify Providers","text":"<ul> <li>Needed for federated access</li> <li>Active Directory or Google Identity Provider are examples</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#access-reports","title":"Access Reports","text":""},{"location":"notes/aws-practitioner/security-compliance/#access-analyzer","title":"Access Analyzer","text":"<ul> <li>Policies within zone of trust allows access outside your zone of trust.</li> <li>Any outside access will be highlighted as a finding</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#credential-report","title":"Credential Report","text":"<ul> <li>List of all IAM users and credentials as a csv</li> <li>Last usage, last password change and MFA setup can also be seen</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#organizational-activity","title":"Organizational Activity","text":"<ul> <li>Select organizational unit/account based user activity and services accessed by the users</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#access-advisor","title":"Access Advisor","text":"<ul> <li>Services that a user can access and which policies are granting the access</li> <li>Last time services were accessed by the user</li> <li>This option sits under a user</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#service-control-policies","title":"Service Control Policies","text":"<ul> <li>SCPs are used with organizations to setup boundaries or permissions. They don't grant permissions</li> <li>Shows SCPs </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#best-practice","title":"Best Practice","text":"<ul> <li>Least privilege access</li> <li>Identity Federation - single sign-on</li> <li>Enable MFA</li> <li>Rotate Credentials regularly</li> <li>Enable IAM Access Analyzer</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#user-dashboard","title":"User Dashboard","text":"<ul> <li>Shows the list of users and pertinent information about the users.</li> <li>Attributes</li> <li>Username</li> <li>Path - organizational structure representations. Paths can also be referred to in policies</li> <li>Group - which groups user belongs to</li> <li>Last login</li> <li>MFA</li> <li>Password age</li> <li>Console last sign-in</li> <li>Access Key ID - Active/Inactive and ID</li> <li>Active Key Age</li> <li>ARN</li> <li>Console Access </li> <li>Signing Certificates</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#aws-kms","title":"AWS KMS","text":"<ul> <li>The Key Management Service (KMS) is a managed service used to store and generate encryption keys that can be used by other AWS services and applications to encrypt and decrypt your data.</li> <li>AWS administrators do not have access to KMS keys and cannot recover deleted keys. </li> <li>KSM keys are for encryption at rest only. SSL is needed for encryption in transit.</li> <li>Integrates with CloudTrail for usage tracking. </li> <li>KMS is region-specific. Each region requires its own instance of KMS. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#components_1","title":"Components","text":"<p>There are 4 key components of AWS KSM:   1. Customer Master Keys (CMK)   2. Data Encryption Keys (DEKs)   3. Key Policies   4. Grants</p>"},{"location":"notes/aws-practitioner/security-compliance/#cmk-types","title":"CMK Types","text":"<ol> <li>Customer Managed - These keys offer the greatest level of flexibility and control.  You are able to create, disable or delete the key, configure the key policies associated with your key, configure Grants, and also alter and adjust the key rotation periods and view full usage history of the key.These keys can be used by other AWS services that integrate with KMS.Customer managed keys include  an additional charge for creating your customer CMKs.</li> <li>AWS  - hese are managed by AWS, however you are still able to view these keys within the Management Console, and also audit and track their usage and view their key policies.However, because they are managed by AWS you are not able to modify them.  These keys are created and used by AWS services that integrate with KMS directly, but they can only be used by the service that creates them.</li> <li>AWS Owned - These are not visible within the KMS console or anywhere within your account, neither do you have the ability to audit and track their usage, they are essentially abstracted from your AWS account.But of course, some services use this key type to encrypt your data within your account.Examples of AWS Owned CMKs include:\u2022The S3 Master key uses SSE-S3 encryption\u2022The default encryption option used on all Amazon DynamoDBtables uses AWS owned keys.</li> </ol>"},{"location":"notes/aws-practitioner/security-compliance/#data-encryption-keys-deks","title":"Data Encryption Keys (DEKs)","text":"<p>Data keys are created by CMKs however they are used outside of KMS to perform encryption against your data, either in your own applications or by other AWS services.</p>"},{"location":"notes/aws-practitioner/security-compliance/#key-policies","title":"Key Policies","text":"<p>Key policies determine who can do what with the key, for example, defining who can use the key to perform cryptographic operations, in addition to those who can administer the CMK to perform functions such as deleting and revoking the key.  Controlling access to CMKs can\u2019t be done using IAM alone. In ALL cases, to manage access to your CMKs, you MUST use a key policy.</p>"},{"location":"notes/aws-practitioner/security-compliance/#grants","title":"Grants","text":"<p>Grants allow you to programmatically delegate your permissions to another principal or user, and so the grant consists of 2 parties, the user who creates the Grant, and the Grantee who then uses that grant to perform cryptographic operations.</p>"},{"location":"notes/aws-practitioner/security-compliance/#sts-security-token-service","title":"STS - Security Token Service","text":"<ul> <li>Request temporary, limited-privilege credentials for IAM users or Federated users.</li> <li>Enabled globally and best practice is to disable regions not being used. </li> </ul> <p>Used for federated user access to roles.  Steps for SAML based role assumption:</p> <ol> <li>A user within an internal organization initiates a request to authenticate against the Active Directory Federated Service, an ADFS server, via a web browser using a single sign on URL. </li> <li>If their authentication is successful by using their Active Directory credentials, SAML will then issue a successful authentication assertion back to the user's client, requesting federated access. </li> <li>The SAML assertion is then sent to the AWS Security Token Service, to assume a role within IAM using the <code>AssumeRoleWithSAML</code> API. </li> <li>STS then responds to the user requesting federated access with temporary security credentials, with an assumed role and associated permissions, allowing S3, EC2, and RDS access as per our example, the user then has federated access to the necessary AWS services as per the role's permissions.</li> </ol>"},{"location":"notes/aws-practitioner/security-compliance/#iam-policies_1","title":"IAM Policies","text":""},{"location":"notes/aws-practitioner/security-compliance/#policy-syntax","title":"Policy Syntax","text":"<p>Represented as JSON and has at least one statement.</p> <ol> <li>Version : Policy language version.</li> <li>Statement : Array of statements is possible.<ol> <li>SID : Unique ID for the policy. </li> <li>Effect : Allow|Deny</li> <li>[NOT] Principal : Who? Only for resource-based policy.</li> <li>[NOT] Action : API Calls to which effect apply</li> <li>Condition : Key,Value pair condition filters that must match.</li> <li>IP Address : <code>aws:sourceIp</code>; 10.0.0.0/16</li> </ol> </li> </ol>"},{"location":"notes/aws-practitioner/security-compliance/#policy-types","title":"Policy Types","text":""},{"location":"notes/aws-practitioner/security-compliance/#identity-based-policies","title":"Identity-based policies","text":"<ul> <li>Policies associated with anything tha depicts and identity</li> <li>Can be of the following variants:</li> <li>Managed Policies - both are saved in library and can be reused across multiple identities:<ul> <li>AWS Management Policies</li> <li>Customer Managed Policies</li> </ul> </li> <li>In-line Policies<ul> <li>Embedded directly into the identity and can't easily be replicated to the identity.</li> </ul> </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#creation","title":"Creation","text":"<ol> <li>Copy existing policy</li> <li>Policy Generator. Use UI and dropdown to create policy. </li> <li>Create your own policy using JSON. This option also has a simple visual editor.</li> </ol>"},{"location":"notes/aws-practitioner/security-compliance/#resource-based-policies","title":"Resource-based policies","text":"<ul> <li>Policies are attached to resources themselves.</li> <li>Has principal attribute to describe who the policy applies to </li> <li></li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#permission-boundaries","title":"Permission Boundaries","text":"<ul> <li>These policies can be associated with a role or user.</li> <li>Don't grant permissions but define the maximum permissions that can be granted. Act as guardrail. </li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#organization-service-control-policies-scps","title":"Organization Service Control Policies (SCPs)","text":"<ul> <li>These policies can be associated Organizations or Organization units. </li> <li>Don't grant permissions but define the maximum permissions that can be granted to users of the Organization or OU. </li> <li>SCPs can be viewed from IAM but can't be edited from IAM and must be done from AWS Organizations services.</li> </ul>"},{"location":"notes/aws-practitioner/security-compliance/#policy-evaluation","title":"Policy Evaluation","text":"<ol> <li>Authentication Check</li> <li>Context (Service and request)</li> <li>Policy Evaluation<ol> <li>Be default all access to a resource is denied</li> <li>Allows need to be explicit</li> <li>Explicit deny will take precedence and will lead to deny</li> </ol> </li> <li>Result of evaluation - Allow/deny</li> </ol> <p>##### Evaluation Order   1. Organizational SCP   2. Resource-based Policies   3. IAM Permission Boundaries   4. Identity-based policies</p>"},{"location":"notes/aws-practitioner/security-compliance/#aws-security-hub","title":"AWS Security Hub","text":"<p>AWS Security Hub is a service that helps customers to improve their security posture on AWS by providing a comprehensive view of security and compliance across their AWS accounts. It aggregates security findings from various AWS services and third-party tools and presents them in a single dashboard. Doing so makes it easier for customers to identify and prioritize security issues and take corrective actions. Moreover, it offers automated compliance checks against industry standards and best practices such as PCI DSS, HIPAA, and CIS AWS Foundations Benchmark. With AWS Security Hub, customers can automate security and compliance checks, eliminate manual processes, and increase the efficiency of their security operations.</p>"},{"location":"notes/aws-practitioner/storage/","title":"Storage","text":""},{"location":"notes/aws-practitioner/storage/#s3","title":"S3","text":""},{"location":"notes/aws-practitioner/storage/#limitations","title":"Limitations","text":"<ul> <li>0 bytes - 5TB per file.</li> <li>100 buckets per account - soft limit. </li> </ul>"},{"location":"notes/aws-practitioner/storage/#properties","title":"Properties","text":"<ul> <li>Object storage service </li> <li>No fixed structure</li> <li>Flat address space, unique URL. </li> <li>Regional service, must specify the Region while upload</li> <li>Automated replication across multiple AZs for durability. </li> <li>Need to create buckets - name must be unique across all of S3. </li> <li>Unique Object key identifies a files in a bucket. Unique Key: Bucket + folder + file.</li> <li>Also, get a URL which is fully qualified</li> <li>Can create virtual folders for easy management</li> <li></li> </ul>"},{"location":"notes/aws-practitioner/storage/#spec","title":"Spec","text":"<ul> <li>Durability - 11 9s</li> <li>Availability - 99.5% - 99.99% (depends on storage class)</li> </ul>"},{"location":"notes/aws-practitioner/storage/#versioning","title":"Versioning","text":"<ul> <li>Keeps all versions of the objects in the same bucket. </li> <li></li> </ul>"},{"location":"notes/aws-practitioner/storage/#server-access-logging","title":"Server Access Logging","text":"<ul> <li>Logs for access to bucket. </li> </ul>"},{"location":"notes/aws-practitioner/storage/#object-level-logging","title":"Object-level logging","text":"<ul> <li>Object-level API activity.</li> </ul>"},{"location":"notes/aws-practitioner/storage/#encryption","title":"Encryption","text":""},{"location":"notes/aws-practitioner/storage/#bucket-level-encryption","title":"Bucket-level Encryption","text":""},{"location":"notes/aws-practitioner/storage/#folder-level-encryption","title":"Folder level Encryption","text":""},{"location":"notes/aws-practitioner/storage/#storage-classes","title":"Storage Classes","text":"S3 Standard S3 INT S3 S-IA S3 Z-IA High throughput Low Latency Frequent access to data Unknown (Frequent (30 days) + Infrequent) Infrequent Infrequent Durability Eleven 9s Eleven 9s across single AZ Availability 99.99% 99.9% 99.9% 9 9.5% SSL/TLS Lifecycle rules"},{"location":"notes/aws-practitioner/storage/#s3-glacier","title":"S3 Glacier","text":"<ul> <li>11 9s durability</li> <li>Lower cost of storage. </li> <li>Retrieval is not instant</li> <li>Vaults and Archives. Vaults are container for Archives. Vaults are region specific. </li> <li>No GUI. GUI is for management only. </li> <li>Moving data two step process - (1) Create Vault (2) Use API/SDK to move data. or via Data lifecycle from S3.</li> <li>Access is via API/SDK or AWS CLI. Either way an archival retrieval job needs to be create first. </li> <li>S3 Access of data comes at a cost based on retrieval options:</li> <li>Set 1<ul> <li>Expedited - &lt;250MB, 5 mins, most expensive</li> <li>Standard - Any size, 3-5 hours, 2nd most expensive</li> <li>Bulk - Any size, 5-12 hours, cheapest</li> </ul> </li> <li>Set 2<ul> <li>Instant Retrieval - milliseconds</li> <li>Flexible Retrieval - mins to hours</li> </ul> </li> </ul>"},{"location":"notes/aws-practitioner/storage/#s3-glacier-deep-archive","title":"S3 Glacier Deep Archive","text":"<ul> <li>Minimal access retrieval is 12 hours.</li> </ul>"},{"location":"notes/aws-practitioner/storage/#fees","title":"Fees","text":"<ul> <li>Minimum storage of 30 days of storage requirement on all tiers, except standard. Glacier has 90 days. Deep Archive 180 days.</li> <li>Per object monitoring fess in Intelligent Tier.</li> <li>Retrieval fees per GB for Infrequent Access and Glacier. </li> <li>S3 Glacier is at a fraction of the cost of S3.</li> </ul> <p>### Lifecycle Rules   * Moving data automatically between different tiers. </p>"},{"location":"notes/aws-practitioner/storage/#ebs","title":"EBS","text":"<ul> <li>Persistent Block-level storage</li> <li>Attached to EC2 volumes and can meet IOPS requirements.</li> <li>Generally, EBS volume can be attached to a single storage (exception Multi-Attach).</li> <li>EBS backup - snapshots. Manually / cloudwatch scheduled event. Stored on S3 and incremental changes are stored. Restored to another EBS volume. </li> <li>Copy snapshot to anther for high durability. </li> <li>Replicated multiple times within the same AZ. Lives only in the single AZ.</li> <li>When creating an EBS Volume directly, need to specify the AZ can only be attached to EC2 instances in the same AZ. </li> </ul>"},{"location":"notes/aws-practitioner/storage/#ebs-types","title":"EBS Types","text":""},{"location":"notes/aws-practitioner/storage/#ssd","title":"SSD","text":"<ul> <li>Better for work with smaller blocks</li> <li>Boot Volume</li> <li>SSD-GP2</li> <li>Balanced </li> <li>Boot Volume</li> <li>Provisioned IOPS SSD - io1</li> <li>Low latency/high throughput</li> <li>16000 IOPS of 250 MiB/s of throughput per volume</li> </ul>"},{"location":"notes/aws-practitioner/storage/#hd","title":"HD","text":"<ul> <li>Throughput optimized HDD (st1)</li> <li>Frequently accesses, large throughput, data streaming, log processing</li> <li>cannot be used as boot volumes</li> <li>Cold HDD (sc1)</li> <li>lowest cost</li> <li>large in size and accessed infrequently</li> <li>cannot be used as boot volumes</li> </ul>"},{"location":"notes/aws-practitioner/storage/#security","title":"Security","text":"<ul> <li>Enable encryption at the time of creation. </li> <li>Uses AWS KMS to automatically encrypt data</li> <li>snapshots of encrypted volumes are also encrypted.</li> <li>Encryption is only available on selected instance types. </li> <li>Regional setting possible to configure all EBS volumes in the region to use encryption. </li> </ul>"},{"location":"notes/aws-practitioner/storage/#efs","title":"EFS","text":""},{"location":"notes/aws-practitioner/storage/#properties_1","title":"Properties","text":"<ul> <li>File Storage Access like a File Manager</li> <li>Uses hierarchy structure</li> <li>Low Latency and multi-access.</li> <li>EFS is accessed via mountpoints that live in particular AZs and can be used by EC2s in that AZ.</li> <li>Uses standard OS API - NFS4.1 and 4.0.</li> <li>Replicated across AZs in a single region, making it highly available and reliable. </li> <li>Regional service. Not available in all regions.</li> </ul>"},{"location":"notes/aws-practitioner/storage/#storage-classes-and-performance","title":"Storage Classes and Performance","text":""},{"location":"notes/aws-practitioner/storage/#standard","title":"Standard","text":"<ul> <li>Default</li> <li>Charged on the storage used at per month.</li> </ul>"},{"location":"notes/aws-practitioner/storage/#infrequent-access","title":"Infrequent Access","text":"<ul> <li>Rarely accessed, lower cost. </li> <li>Higher latency of first byte read. </li> <li>Cost for storage space used + Cost for read and write. </li> </ul>"},{"location":"notes/aws-practitioner/storage/#lifecycle","title":"Lifecycle","text":"<ul> <li>Moves data between the storage classes.</li> <li>Fixed duration of timers 14, 30, 60, 90 days. </li> <li>Metadata and files smaller than 128K in size. </li> <li>Can be switched on / off. </li> </ul>"},{"location":"notes/aws-practitioner/storage/#performance-modes","title":"Performance Modes","text":"General Purpose Max I/O Throughput Standard Unlimited IOPS &lt;= 7K &gt;= 7k Latency Low Latency High Latency Metric Cloudwatch % of IPOS"},{"location":"notes/aws-practitioner/storage/#throughput-modes","title":"Throughput Modes","text":"Bursting Provisioned Throughput 100MiB/s per TB Additional charges beyond the bursting option"},{"location":"notes/aws-practitioner/storage/#duration-of-burst","title":"Duration of Burst","text":"<ul> <li>50 MiB/s per TB is baseline, </li> <li>lower accumulates burst credit </li> <li>burst credit  is viewable on CloudMetrics.</li> </ul>"},{"location":"notes/aws-practitioner/storage/#snow-family","title":"SNOW Family","text":"<ul> <li>Storage and compute capabilities.</li> <li>Few TB to 100PB of physical data transfer in and out of AWS.</li> </ul> Snowcone Snowball Compute Optimized Snowball Compute Optimized with GPU Snowball Storage Optimized Snow Mobile vcpu 2 52 52 24 n/a memory 4GB 208GB 208GB 32GB n/a storage 8TB 39.5TB 39.5TB 80TB 100PB SSD n/a 7.68 7.68 n/a n/a GPU n/a n/a Nvdia Tesla n/a n/a cluster na 5-10 5-10 5-10 n/a use cases portable, battery, upto 8TB, DataSync, 10Gbit S3 API, 100Gbit network compute power, cluster 80TB, SSD, Rack mounting, HIPPA compliant 10 petabyte"},{"location":"notes/aws-practitioner/storage/#faq","title":"FAQ","text":""},{"location":"notes/aws-practitioner/streaming/","title":"Streaming Data - Amazon Kinesis","text":"<ul> <li>No Free Tier, each service has its own pricing based on attributes.</li> </ul>"},{"location":"notes/aws-practitioner/streaming/#amazon-kinesis-data-streams","title":"Amazon Kinesis Data Streams","text":""},{"location":"notes/aws-practitioner/streaming/#properties","title":"Properties","text":"<ul> <li>real-time data collection messaging service</li> <li>Maintains a copy of all data in the received order</li> <li>Data is stored for 24 hours default, upto 65 days if configured using <code>IncreaseStreamRetentionPeriod</code>.</li> <li>Ability to process large data streams in real-time, ability to read and replay records to multiple consumer applications. </li> <li>Kinesis Data Stream can encrypt data using KMS as producers put data on the stream.</li> <li>Kinesis data stream has put to get latency of less than 1 seconds.</li> </ul>"},{"location":"notes/aws-practitioner/streaming/#structure","title":"Structure","text":"<ul> <li>Stream is made of one or more Shards</li> <li>Shards hold Data Records in sequence. Shards can handle 1000 records/second. Write - 1000 records/second 1MB/second. Read - 1MB/seconds. </li> <li>Data Record is made of Partition Key (which shard), Sequence Number (per shard ordering) and Actual Data (up to 1MB). </li> <li>Producers put data on streams and consumers read data using Kinesis Client Library (KCL).</li> <li>Kinesis Data Stream does not inspect or alter the data payload, consumers must do what is necessary. </li> <li>Data in streams are immutable.</li> <li>Data can't be removed and can only expire.</li> </ul>"},{"location":"notes/aws-practitioner/streaming/#kinesis-client-library-kcl","title":"Kinesis Client Library (KCL)","text":"<ul> <li>KCL manages a Record Processor per shard ensuring data processed from each Shard.</li> <li>KCL uses DynamoDB to store control data and creates one table per application reading data from a stream.</li> <li>Can run on EC2, Elastic Beanstalk and Data Centre Servers. </li> </ul>"},{"location":"notes/aws-practitioner/streaming/#configuration","title":"Configuration","text":""},{"location":"notes/aws-practitioner/streaming/#on-demand","title":"On Demand","text":"<ul> <li>Adjusts throughput based on demand</li> <li>Billed for actual usage. </li> </ul>"},{"location":"notes/aws-practitioner/streaming/#provisioned","title":"Provisioned","text":"<ul> <li>Specify the number of shards</li> <li>Billed for hourly provision of shards</li> <li>Splitting and Merging shard operations happens during increase and decrease.</li> </ul>"},{"location":"notes/aws-practitioner/streaming/#consumer","title":"Consumer","text":""},{"location":"notes/aws-practitioner/streaming/#classic","title":"Classic","text":"<ul> <li>Pulls data from streams</li> <li>Limits of how many times consumers can pull</li> </ul>"},{"location":"notes/aws-practitioner/streaming/#enhanced-fan-out","title":"Enhanced Fan out","text":"<ul> <li>Every consumer gets 2 MBPs throughput per shard and limits are removed. </li> </ul>"},{"location":"notes/aws-practitioner/streaming/#amazon-kinesis-video-streams","title":"Amazon Kinesis Video Streams","text":"<ul> <li>Stream processing binary-encoded data such as video and audio. </li> <li></li> </ul>"},{"location":"notes/aws-practitioner/streaming/#amazon-kinesis-data-firehose","title":"Amazon Kinesis Data Firehose","text":"<ul> <li>Streaming delivery service</li> <li>Data can be dynamically transformed and scaled automatically to deliver to data store. Can transform to Parquet from JSON. Can use Lambda for transformation.</li> <li>No need to develop custom consumers</li> <li>Buffers data during delivery, Buffer Size and Buffer Duration can be configured during creation. This buffering makes it near real-time. </li> <li>Destinations:</li> <li>Amazon S3</li> <li>Amazon Redshift</li> <li>Amazon Elasticsearch</li> <li>Splunk</li> <li>Generic HTTP endpoints</li> <li>DataDog</li> <li>MongoDB Cloud</li> <li>New Relic</li> <li>No free tier, charged for usage not provisioned. </li> </ul>"},{"location":"notes/aws-practitioner/streaming/#amazon-kinesis-data-analytics","title":"Amazon Kinesis Data Analytics","text":"<ul> <li>Read from stream real-time and analytics on streams</li> <li>Only SQL available when used with Firehose</li> <li>Apache Flink with Java and Scala are available with only Data Streams.</li> </ul>"},{"location":"notes/aws-practitioner/summary/","title":"Summary","text":"Topic Area Description Cost Explorer Cost Detailed view of Cost. Base Free, Granular + API charged. Billing Dashboard Cost Detailed view of Bills and cost from major services CUR Cost Default OFF, detailed cost usage report ar varying granularity. Stored to S3, accessible by Athena, Quicksight, Redshift. AWS Budget Cost Automatic Notification and Actions when set thresholds are hit- cost, usage, reservation, savings plans. Action via SCP and IAM. AWS Tag Editor Cost, Reporting Updating tags in bulk. Compute Savings Plan Purchase, Cost Across all compute services, AZs, instance families, regions. Full flexibility. 66% EC2 Instance Savings Plan Purchase, Cost Across EC2 of same instance families in defined region.  72% AWS OpsWork Config AWS OpsWorks is a configuration management service that helps customers configure and operate applications, both on-premises and in the AWS Cloud, using Chef and Puppet. CodeDeploy CICD AWS CodeDeploy automates code deployments to any instance, including Amazon EC2 instances and instances running on-premises. AWS Security Hub Security Automated compliance checks against industry standards and best practices such as PCI DSS, HIPAA, and CIS AWS Foundations Benchmark. CloudEndure DR tool that minimizes downtime and data loss by providing fast, reliable recovery of physical, virtual, and cloud-based servers into AWS Cloud. AWS Device Farm CICD In AWS, you can test your app against a massive collection of physical devices in parallel. AWS Ground Station Other service is for controlling satellite communications and processing data using satellites. Amazon QuickSight BI a business intelligence service for creating visualizations and dashboards. AWS Security Bulletin Doc security announcement provider service. AWS SES (Simple Email Service) Notification Used to send only emails out of AWS AWS CodeDeploy CICD Automate Code Deployments to instances including EC2 AWS CodePipeline CICD Continuous Code Delivery Pipeline AWS CodeCommit CICD Fully-managed source control service AWS CodeBuild CICD Fully Managed continuous integration service that compiles source code, run tests and produce software packages ready to deploy. AWS CodeStar CICD Quickly Develop, Build, and Deploy Applications on AWS. AWS SWS (Simple Workflow) Workflow The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that coordinate work across distributed components. AWS Comprehend Other, NLP Analyze and recognize entities and connectivity between text Amazon Textract Other, IR This service is mainly used to extract printed text, handwriting, and data from virtually any document. Amazon Monitron Other is a machine learning service used for detecting abnormal industrial machine behavior and it enables you to implement predictive maintenance. AWS Connect Other a seamless experience across voice and chats for your customers and agents. AWS Lex Other It is used to build chatbots for everyday consumer requests, NLP + AI AWS Transcribe Other This service turns speech into text AWS Polly Other This service turns text into speech AWS Resource Access Manager Management, Access is used to securely share your resources across your AWS accounts. Session Manager Dev Tool fully managed AWS Systems Manager capability that lets you manage your EC2 instances, on-premises instances, and virtual machines (VMs) through an interactive one-click browser-based shell or through the AWS CLI. EC2 Instance Connect Dev Tool connect to your Linux instances using a browser-based client. AWS Cost Anomaly Detection Cost helps you detect and alert on any abnormal or sudden spend increases in your AWS account. Amazon WorkSpaces Other is simply a secure cloud desktop service. Both Linux and Windows supported. AWS Migration Hub Migration can only monitor application migrations. AWS Database Migration Service Migration is mainly used for migrating databases from on-premises to AWS. AWS Application Migration Service (MGN) Migration is the primary migration service recommended for lift and shift migrations to AWS, moving workloads. AWS Server Migration Service (SMS) Migration this is simply an agent-less service that makes it easier and faster for you to migrate thousands of on-premises workloads to AWS. Amazon AppStream 2.0 Other is a fully managed application streaming service that provides users with instant access to their desktop applications from anywhere. AWS AppSync Data, API is mainly used to develop GraphQL APIs using serverless managed endpoint with data from multiple sources. AWS Data Exchange Data it is a service that makes it easy for millions of AWS customers to securely find, subscribe to, and use third-party data in the cloud AWS PrivateLink Networking is a highly available, scalable technology that lets you privately connect your VPC to services as if they were in your VPC. AWS Config Config this service is mainly used to evaluate the configuration settings of your AWS resources. AWS Wavelength Other Brings AWS services to the edge of the 5G network, minimizing the latency to connect to an application from a mobile device AWS DataSync Data Transfer can quickly move large amounts of data across the file system, object storage, Amazon S3, Amazon EFS, and Amazon FSx for Windows file servers and On-prem. AWS Transfer Family Data Transfer allows your files to be transferred through secure file transfer protocols (SFTP), FTPS (FTP over SSL/TLS), and FTP(unencrypted) AWS Systems Manager Config this service is used for the automation of your EC2 instances. Amazon Lightsail App Deployment service is just a package solution for the fast deployment of websites and other applications. Amazon Mechanical Turk Other an on-demand, scalable, human workforce to complete jobs that humans can do better than computers AWS Signer Other is a code-signing service to ensure the trust and integrity of your code. APN Technology Partner Support An AWS Technology Partner company provides software tools and services hosted on or integrated with AWS. APN Consulting Partner Support The Consulting Partner helps an AWS customer in the implementation and management of an AWS cloud deployment. AWS Trusted Advisor Architecture Cost Optimization, Performance, Security, Fault Tolerance, Service Limits AWS Proton Workflow is a deployment workflow tool. It is mainly used to standardize infrastructure and automate the deployment of serverless &amp; container-based applications. AWS License Manager Security allows you to manage software licenses from different vendors. AWS Systems Manager Parameter Store Security used to centralize the configuration data of their application. You can store data such as passwords, database strings, AMI IDs, and license codes as parameter values. Amazon Chime Other is a high-quality communications service that transforms online meetings with an easy-to-use app that works seamlessly across all your devices. AWS Well-Architected Framework Architecture ORSPC - Operational Excellence, Reliability, Security, Performance and Cost Optimization"},{"location":"research/cloud-security/","title":"Automated Security Transparency","text":""},{"location":"research/cloud-security/#overview","title":"Overview","text":"<p> Automated Security Transparency Tool (ATT) is our automated security transparency tool. ATT is written in Java which makes it highly portable and platform independent. Figure on the right shows the system architecture of ATT. The main tasks performed by various components of the tool are to read ACR specification and generate a security matrix, enable logging of the resource interactions, monitors resource interactions, identify violations against the security matrix and send notifications in the event of violations.</p> <p>The current version of ATT is able to parse ACR statements written in ASRL . ATT provides options for importing plain text files containing ASRL and provides significant feedback on syntax errors in order to aid resolve any specification errors. Once a specification file is successfully parsed, ATT generates an in-memory security matrix corresponding to the file. This security matrix is vital to the monitoring operation.</p> <p>Following creation of the security matrix, the tool interfaces with the user interaction logging component and starts the logging process. As an initial implementation, we used LoggedFS as our logging system. LoggedFS is a FUSE based filesystem which can log every operation to the underlying file system in real-time and as it happens. Once the logging process is enabled, any request for interaction to monitored resources generates a detailed logged entry which is consumed directly by ATT enabling real-time usage monitoring. The monitoring is done against security matrix and any violation of defined ACRs or attempted breach is identified.</p>"},{"location":"research/cloud-security/#att-download","title":"ATT Download","text":"Download Link Description ATT ATT Distribution."},{"location":"research/cloud-security/#system-requirements","title":"System Requirements","text":""},{"location":"research/cloud-security/#java-requirements","title":"Java Requirements","text":"<p>Java_1_8 is required to run ATT.</p> <p>Fortunately, its quiet easy to get it running on Linux even without root. Just do the following:</p> <ol> <li>Download the JDK as a tarball from Oracle</li> <li>Unzip it somewhere in your HOME (for instance,\u2006HOME/jdk).</li> <li>Set JAVA_HOME to the path of the root JDK install</li> </ol> <p>For example,  1. <code>export JAVA_HOME=/cs/research/crest/home0/sislam/opt/jre1.8.0_65</code>  2. <code>export PATH=\"$JAVA_HOME/bin:$PATH\"</code></p> <p>The current version of ATT is also limited to Linux Distributions as it requires supporting tools which are detailed below.</p>"},{"location":"research/cloud-security/#utility-requirements","title":"Utility Requirements","text":"<p>You need to have the following tools installed on your machine:  1. FUSE  2. LoggedFS</p> <p>Most Linux Distribution will come with FUSE. On Ubuntu you can use the following commands to install FUSE:</p> <pre><code>  sudo apt-get update\n  sudo apt-get install librlog-dev libfuse-dev libxml2-dev libpcre3-dev\n</code></pre> <p>If Loggedfs is included in your distribution you can just install with your package manager. Otherwise follow instructions at LoggedFS website to install from source.</p>"},{"location":"research/cloud-security/#troubleshooting-and-help","title":"Troubleshooting and Help","text":"<p>If you are having trouble running the tool or need any help with installation please contact me.</p>"},{"location":"research/fastfix/","title":"Search-based Software Engineering (SBSE)","text":""},{"location":"research/fastfix/#fastfix-automated-bug-triage","title":"FastFix - Automated Bug Triage","text":""},{"location":"research/fastfix/#overview","title":"Overview","text":"<p>This page provides all the datasets and the tools for building the sum-based developer-term association matrix and using it for performing bug assignments.</p> <p>The work here is supported by EU project FITTEST (ICT-2009.1.2 no 257574) and EU project FastFix (FP7- 258109).</p> <p>We would also like to thank SFT and the ATLAS team at CERN for making the data available to us.</p>"},{"location":"research/fastfix/#bug-report-data","title":"Bug Report Data","text":""},{"location":"research/fastfix/#combined-dataset","title":"Combined Dataset","text":"<p>Please click link below to download sql dumps for all projects together.</p> <ul> <li>Filtered dataset containing bug reporting activity.</li> <li>Unfiltered dataset containing bug reporting activity.</li> </ul>"},{"location":"research/fastfix/#individual-datasets","title":"Individual Datasets","text":""},{"location":"research/fastfix/#filtered","title":"Filtered","text":"<ul> <li>Atlas</li> <li>Belle2</li> <li>Eclipse</li> <li>Unicase</li> </ul>"},{"location":"research/fastfix/#unfiltered","title":"Unfiltered","text":"<ul> <li>Atlas</li> <li>Belle2</li> <li>Eclipse</li> <li>Unicase</li> </ul>"},{"location":"research/program-analysis/","title":"Program Analysis Projects","text":""},{"location":"research/program-analysis/#decluvi-dependence-cluster-visualisation","title":"deCluVi: Dependence Cluster Visualisation","text":""},{"location":"research/program-analysis/#overview","title":"Overview","text":"<p>deCluVi is a multi-level software cluster visualisation tool. The tool has two graph based visualizations B-SCG and the F-SCG. Four other views, starting with the most abstract are the heat-map view, system view, line view and finally the code view. The tool is designed to be interactive and each of these views are presented in its window for ease in comparison and to maximize available screen space. Sliders and quick snap buttons allow quick selection of particular and a range of clusters. Dynamic coloring, range filtering and dead code elimination options make it easier to visualize and understand the software clusters. A paper on cluster visualization gives more detail about the tool.</p> <p>The tool and the data set for visualising many GNU programs are given below. The scheme scripts for extracting data from CodeSurfer is also provided if you wish to analyze other programs. After starting the tool you will need to select the scheme output file which can be found in the source directory for each of the programs.</p>"},{"location":"research/program-analysis/#screen-shots","title":"Screen Shots","text":""},{"location":"research/program-analysis/#tool-download","title":"Tool Download","text":"<p>The current version of the deCluVi can be downloaded from here.</p> <p>Visualisation data for transformed versions of GNU programs for coverage based clustering:</p> <ul> <li>bc-1.06.tar.gz</li> <li>byacc-20100610.tar.gz</li> <li>cflow-1.3.tar.gz</li> <li>ed-1.2.tar.gz</li> </ul>"},{"location":"research/program-analysis/#scheme-script","title":"Scheme Script","text":"<p>The scheme script to extract slice data from Code Surfer can be downloaded from here. (Please note that the script is compatible with version 2.1 of Code Surfer)</p>"},{"location":"research/program-analysis/#observation-based-program-slicing-orbs","title":"Observation-based Program Slicing (ORBS)","text":""},{"location":"research/program-analysis/#overview_1","title":"Overview","text":"<p> Current slicing techniques cannot handle systems written in multiple programming languages. Observation-Based Slicing (ORBS) is a language-independent slicing technique capable of slicing multi-language systems, including systems which contain (third party) binary components. A potential slice obtained through repeated statement deletion is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted. The resulting slice is similar to a dynamic slice.</p> <p>We evaluate five variants of ORBS on ten programs of different sizes and languages showing that it is less expensive than similar existing techniques. We also evaluate it on bash and four other systems to demonstrate feasible large-scale operation in which a parallelised ORBS needs up to 82% less time when using four threads. The results show that an ORBS slicer is simple to construct, effective at slicing, and able to handle systems written in multiple languages without specialist analysis tools.</p>"},{"location":"research/program-analysis/#porbs-download","title":"PORBS Download","text":"<p>PORBS is currently distributed as JAR executable file. You can download any of the following which has the tool distribution along with the required scripts to build and execute the system being sliced. Each download comes with a README explaining how to run the example.</p> Download Link Description Example Multi-language System written in Java, Python and C. Setup to slice on variable dot. Sort Uniq Utility Sort and Uniq utility written in Java. Configure to slice with input file and option \"-s\" Sort Uniq Utility Sort and Uniq utility written in Java. Configure to slice with input file and option \"-u\" Word Count Word Count example, line 45, variable c. Word Count Word Count example, line 29, variable inword."},{"location":"research/program-analysis/#system-requirements","title":"System Requirements","text":""},{"location":"research/program-analysis/#java-requirements","title":"Java Requirements","text":"<p>Java_1_8 is required to run PORBS.</p> <p>Fortunately, its quiet easy to get it running on Linux even without root. Just do the following:</p> <ol> <li>Download the JDK as a tarball from Oracle</li> <li>Unzip it somewhere in your HOME (for instance,\u2006HOME/jdk).</li> <li>Set JAVA_HOME to the path of the root JDK install</li> </ol> <p>For example, <pre><code>export JAVA_HOME=/cs/research/crest/home0/sislam/opt/jre1.8.0_65\nexport PATH=\"$JAVA_HOME/bin:$PATH\"\n</code></pre></p>"},{"location":"research/program-analysis/#utility-requirements","title":"Utility Requirements","text":"<p>The compile and execute script will need the following to run correctly. Note that some specific systemm will have additional requirements. Please refer to README of the download for more details.</p> <p>For example,</p> <ol> <li><code>bash</code></li> <li><code>time</code></li> <li><code>timeout</code></li> <li><code>KILL</code></li> <li><code>md5sum</code></li> <li><code>sed</code></li> </ol> <p>Most Linux Distribution will come with these utilities installed as default.</p> <p>Mac OS</p> <p>MAC OS does not have these utilities installed by default. You have to either adapt the scripts or install the utilities.</p>"},{"location":"research/program-analysis/#troubleshooting-and-help","title":"Troubleshooting and Help","text":"<p>If you are having trouble running the tool or need any help with installation please contact me.</p>"},{"location":"research/program-analysis/#further-information","title":"Further Information","text":"<p>Further information about ORBS and the sequential ORBS slicing tool is available from the official ORBS Website.</p>"},{"location":"research/program-analysis/#energy-measurement-and-program-slicing","title":"Energy Measurement and Program Slicing","text":""},{"location":"research/program-analysis/#explanation-of-the-data-format","title":"Explanation of the data format","text":"<p>The intial dataset for the subject program can be downloaded from below. Each download gives you a tarball with roughly the following directory structure:</p>"},{"location":"research/program-analysis/#config","title":"config","text":"<ul> <li><code>setup.sh</code> - setup script for the project, should setup the project for execution. For example: <code>sh config/setup.sh</code> should setup the code in a directory called orig ready for execution and compilation.</li> <li><code>compile.sh</code> - compile script for the project. For example: <code>sh config/compile.sh orig</code> should compile the code in corresponding orig directory.</li> <li><code>execute.sh</code> - execute script for the project. For example: <code>sh config/execute.sh orig</code> should execute the code in corresponding orig directory and show output.</li> </ul>"},{"location":"research/program-analysis/#orig","title":"orig","text":"<p>Contains the original source files which of the subject program to be tested.</p>"},{"location":"research/program-analysis/#regression","title":"regression","text":"<p>Contains the output of the slicer. Each directory in regression is the resulting output of running the slicer on variable v in source file f at line l. For instance, in the program Example you will find the directory <code>regression/checker_java_14_dots-8</code>, where</p> <ul> <li><code>checker_java</code> - is the name of the file f</li> <li><code>14</code> - line number l</li> <li><code>dots</code> - variable v</li> <li><code>8</code> - window size for the slicing configuration (ignore this for now)</li> </ul>"},{"location":"research/program-analysis/#description-of-log-files","title":"Description of Log Files","text":"<p>The regression directory also contains the following log files which gives you detailed information about the execution of ORBS and how it progresses:</p> <ul> <li><code>orbs.log</code> - Contains the execution attempt that ORBS does at each line and the result of that execution effort.<ul> <li>Headers - The first part of the log file gives details of the version of the Parallel ORBS tool in use followed the the configuration specific to the execution.</li> <li>Scipts - This is followed by a copy of the compile, execute and the setup scripts in use during this execution</li> <li>Oracle - The line with the header <code>Oracle:</code> contains the md5sum of the output that ORBS is using as oracle and will try to match.</li> <li>Sanity Runs - The log file then includes <code>Sanity Run [X]</code> which are there to ensure that the oracle can be achieved by the ORBS framework.</li> <li>Marking Blank Lines as Deleted - The current version makes a pass over the entire file and marks all blank lines as deleted. <code>Marking blank lines as deleted</code> shows the lines removed from each file that ORBS is working with.</li> <li>Orbs Execution - This is followed by the ORBS execution and attempt at deletion of the lines and their outcome. The explanation of the line <code>I:1 F:checker.java L:14 DWS:2 D [14 13]</code> is as follows:<ul> <li><code>I:1</code> - <code>I</code> stands for iteration number. The <code>1</code> shows that this is iteration 1 for ORBS</li> <li><code>F:checker.java</code> - <code>F</code> stands for file name. <code>checker.java</code> is the file from which ORBS was attempting deletion</li> <li><code>L:14</code> - <code>L</code> stands for line number. <code>14</code> is the line number where the deletion attempt took place</li> <li><code>DWS:2</code> - <code>DWS</code> stands for Deletion Window Size. The number <code>2</code> indicates that deletion window size of 2 was successful and hence two lines were removed. DWS:0 would indicate no deletion was successful at this line.</li> <li><code>[14 13]</code> - Indicates the lines that were deleted. In this case lines <code>14</code> and <code>13</code> were successfully deleted and replaced with a blank line. Please note that this only shows up if there were lines that were successfully deleted by ORBS.</li> </ul> </li> <li>Statistics -  At the end statistics such as Total Compilation, Total Executions, Total Cached Compilations, Total Cached Executions, Lines Deleted and Total Lines are output.</li> </ul> </li> <li><code>DeletePattern.log</code> - This file prints a pattern of the removal that took place. The format of the file is as follows:<ul> <li>The first line of the log file indicates the order of the files which were operated on by ORBS.</li> <li>The second line is the line where ORBS makes a pass to mark all blank lines as removed.</li> <li>Each of the following rows represents an iteration of ORBS on the files.</li> <li>The symbol <code>|</code> is used to delimit the operation on each of the files.</li> <li>The symbol <code>-</code> represents unsuccessful attempt to remove line.</li> <li>The symbol <code>.</code> means line already removed in previous iteration.</li> <li>The symbols <code>a-z</code> represents number of removed lines. <code>a</code> means 1 line removed while <code>eeeee</code> means 5 lines removed.</li> </ul> </li> </ul>"},{"location":"research/program-analysis/#data-download","title":"Data Download","text":"<ul> <li>Example</li> <li>Word_Count_1</li> <li>Word_Count_2</li> </ul> <p>Apologies</p> <p>I realized that this naming convention above is quiet poor! Please excuse it for now and I will put in a fix with a more intuitive naming convention for the upcoming data.</p>"},{"location":"research/project_ideas/","title":"Project Ideas for Students","text":"<p>This page is dedicated to listing and describing a few of the project ideas that I have. Depending on the project they will be suitable for different levels of students from UG to PhDs.</p> <p>I am interested in supervising undergraduate and postgraduate projects. If you are interested in projects that are research oriented, have practical technical components to them and are related to software engineering, please feel free to contact me to discuss your ideas.</p> <p>I am also interested in PhD supervision in the fields of software engineering, dependence analysis, search-based software engineering and program analysis. I am also happy to co-supervise in other areas.</p> <p>You may want to look into my research interests or specific project ideas below to see if there is anything that is of interest to you.</p>"},{"location":"research/project_ideas/#areas-of-interest","title":"Areas of Interest","text":"<ul> <li>Software Engineering</li> <li>Software Development</li> <li>Machine Learning</li> <li>Program Analysis</li> <li>Code Metrics</li> <li>Static Analysis</li> <li>Dynamic Analysis</li> <li>Search-based Software Engineering</li> <li>Software Visualisation</li> </ul>"},{"location":"research/project_ideas/#project-topics","title":"Project Topics","text":"<p>Click on the links below to view details of topic.</p> Project Title Entailing Areas Annoying Captcha! Image processing, Speech processing How much code are you writing? Program Analysis, Software Metrics Web Portal for ORBS Web Application Development Improving slicing performance through binary masking Program Trasformation, Re-writing, Static Analysis, Dynamic Analysis Binary-level program Slicing Program Trasformation, Re-writing, Static Analysis, Dynamic Analysis, Binary Analysis Software Repository Mining Data mining, Software repository mining, Data Analysis, Software Metrics\\ App Store Mining Requirement Analysis, Next release problem, Data mining, Software repository mining, Data Analysis, Software Metrics Search-based Software Slicing  Search-based Software Engineering, Genetic Algorithm, Program Slicing, Static Analysis, Dynamic Analysis Path to riches - Algorithimic Trading Trading Algorithms, Automated Trading, Metatrader, High Performance Trading, Technical Analysis, Machine Learning, Evolutionary Algorithms Automated Bug Triage Machine Learning, Data Analysis, Natural Language Processing, Latent Dirichlet allocation, Support Vector Machine Automated Assessment Web Application Development, Sandboxing, Software Security, Plagiarism Detection, Clone Detection, Code Provenance Software Visualisation Software Visualisation, Web Systems Development, JavaScript, D3 Improving battery life on your phone Mobile App Development, Energy Measurement Energy Consumption in the Cloud Cloud Computing, Energy Measurement Smart Scheduling Evolutionary Algorithms, Genetic Algorithms, Machine Learning, Search-based Software Engineering, Hill Climbing, Scheduling, Web Application Development Smart Tokenized ORBS Program Trasformation, Re-writing, Static Analysis, Dynamic Analysis, Program Slicing Delta Orbsing Delta Debugging, Delta, Program Trasformation, Re-writing, Static Analysis, Dynamic Analysis, Program Slicing Brain Region Routing Problem Steiner Minimal Tree, Graph Theory, Networks, Shortest Path, Routing Problem, Web Application Development, Software Visualisation Anti-Money Laundering System Compliance, Natural Language Processing, NoSQL, Scrapping,"},{"location":"research/project_ideas/#annoying-captcha","title":"Annoying Captcha?","text":"<p>I regard Captcha's as the necessary evil. Modern web systems are littered with Captchas and they are required in many cases to avert bots that try to get into the system. However, there are some instances where I find them very annoying, I always wanted to look into breaking Captchas but never had the time.</p> <p>This project will look into the current techniques to breaking Captchas and extend the state of the art with techniques and tools for breaking captchas?</p>"},{"location":"research/project_ideas/#how-much-code-are-you-writing","title":"How much code are you writing?","text":"<p>Everyday programmers write code, but how much of the code written is actually new. For example, if you were write a small program in Java, can you actually write something that hasn't been written before? For instance consider the following program:</p> <p>Hello.java<pre><code>public class Hello {\npublic static void main (String[] args){\nSystem.out.println(\"Hello, World\");\n}\n}\n</code></pre> How much do you think can be found on the web? This is the result of searching for the code. </p>"},{"location":"research/project_ideas/#web-portal-for-orbs","title":"Web Portal for ORBS","text":"<p>In computer programming, program slicing is the computation of the set of programs statements, the program slice, that may affect the values at some point of interest, referred to as a slicing criterion. Program slicing can be used in debugging to locate source of errors more easily. Observation-based slicing (ORBS) is a novel language-independent slicing technique that revives the original spirit of program slicing, that is, remove/delete parts of the program that does not contribute to the point of interest. A potential slice is obtained through repeated deletion which is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted.</p> <p>This project will gather requirements to release ORBS as a web portal where people can submit their code and later visualise and download slicing information for their systems.</p>"},{"location":"research/project_ideas/#improving-slicing-performance-through-binary-masking","title":"Improving slicing performance through binary masking","text":"<p>Program transformation and automatic re-writing tools play a vital part in research. A tool that allows you to automatically add a mask at each line of code to make the line conditionally executable based on some external environment variable would have many experimental applications in program analysis research. One area where this could be immediately applied to is program slicing, in particular ORBS.</p> <p>This project will develop a program transformation re-writing tool where each line of code can be made conditionally executable based on some external environment variable.</p>"},{"location":"research/project_ideas/#binary-level-orbs-slicing","title":"Binary-level ORBS Slicing","text":"<p>In computer programming, program slicing is the computation of the set of programs statements, the program slice, that may affect the values at some point of interest, referred to as a slicing criterion. Program slicing can be used in debugging to locate source of errors more easily. Observation-based slicing (ORBS) is a novel language-independent slicing technique that revives the original spirit of program slicing, that is, remove/delete parts of the program that does not contribute to the point of interest. A potential slice is obtained through repeated deletion which is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted.</p> <p>This project will apply the ORBS approach at binary level. For this student will need to identify current binary re-writing techniques and tools and use that in conjunction with ORBS to develop a binary slicer.</p>"},{"location":"research/project_ideas/#software-repository-mining","title":"Software Repository Mining","text":"<p>Open-source code and projects are now in abundance more than before. These repositories (eg. Github, bitbucket etc.) provide wealth of information about actual real-world software development. For example, we can understand how code evolved over time, how bugs that were reported were fixed, what techniques and methodology the software development process followed etc. We can even work to find similarities in the repositories identify a host of code metrics.</p> <p>This project will identify the current tools and technologies available for mining these open software repositories and will look to develop and extend them to allow improved data mining and analysis.</p>"},{"location":"research/project_ideas/#app-store-mining","title":"App Store Mining","text":"<p>The software development domain is always evolving, with new paradigms being continually introduced. Since, the launch of the iPhone in 2007 the concept of smart phones have really taken off. With ~967.78m smart phone sold there is no shortage of application users. The market is dominated by three stores where apps are available for download, namely, Apple Store, Google Play and Blackberry Store. These store have a huge number (~1m) of apps available, often with description and various other information related to the app and in some cases even reviews. They also have a price associated with them. Analysis of this data will allow us to gain an insight into the current software development market and trend.</p> <p>This project will identify the current tools and technologies available for crawling app stores and will look to develop and extend them to allow app store data mining and analysis.</p>"},{"location":"research/project_ideas/#search-based-software-slicing","title":"Search-based Software Slicing","text":"<p>In computer programming, program slicing is the computation of the set of programs statements, the program slice, that may affect the values at some point of interest, referred to as a slicing criterion. Program slicing can be used in debugging to locate source of errors more easily. Current program slicing techniques cannot handle systems written in multiple programming languages. Observation-based slicing (ORBS) is a novel language-independent slicing technique capable of slicing multi-language systems including systems which contain (third party) binary components. A potential slice obtained through repeated statement deletion is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted.</p> <p>This project will use the ORBS approach to develop a search-based approach to program slicing.</p>"},{"location":"research/project_ideas/#algorithimic-trading","title":"Algorithimic Trading","text":"<p>The Forex Trading market is the largest and the most liquid market in the world with and estimated $5.3 Trillion being traded every single day. This market is rife with automatic trading algorithms that are trading to make a profit.</p> <p>This project will investigate the current approaches and tools available for automatic trading and develop algorithms and tools to support automated trading, with an aim to make profit, of course! The approaches can come from various domains of computing including machine learning and evolutionary algorithms etc.</p>"},{"location":"research/project_ideas/#automated-bug-triage","title":"Automated Bug Triage","text":"<p>Most large software development projects use some form of bug tracker system. Some popular ones are Jira, YouTrack, IBM Rational, Bug-Track, Bugzilla etc. When a new bug report is filed, a human needs to look at the bug report and triage the bug in order to identify whether its an actual bug. The triage process then tries to identify a developer who the bug should be assigned to. Although this approach works pretty well for software companies where the manager does the assignment and is aware of the capabilities of his team, the scenario is very different for open systems.</p> <p>This project will develop tools and support for automatic bug triage and assignment to developers. This would incorporate areas such as data mining, machine learning, search-based optimisation techniques.</p>"},{"location":"research/project_ideas/#automated-assessment","title":"Automated Assessment","text":"<p>Almost all Computer Science related courses taught at universities will have modules where programming languages are taught. In the case of UEL we have courses taught in Java and php. Often, it is helpful if a automated assessment system can be employed to test and grade programming assignments submitted by students.</p> <p>This project will investigate the current tools and software support available for automated assessment of programming assignments and extend support. This is an ideal project for someone who is interested in web application development. If this project is successful we can look towards a live implementation for the department.</p>"},{"location":"research/project_ideas/#software-visualisation","title":"Software Visualisation","text":"<p>Software visualisation helps us see information and understand data very easily. In moder times we can take advantage of HTML5 and progress in browser technology create very useful and attractive visualisations.</p> <p>This project will develop a graph visualiser in support of processing and viewing various data and graphs.</p>"},{"location":"research/project_ideas/#energy-measurement-for-mobile-platforms","title":"Energy Measurement for Mobile Platforms","text":"<p>Traditionally software development has placed a lot of emphasis on functional requirements. However, in modern software systems non-functional requirements play a vital role, for example, for mobile applications it is very important to know the amount of energy consumed by a piece of software. The software on my iPhone will not be of much use if my phone runs out of battery.</p> <p>The goal of the project will be identify the current state-of-the-art in energy measurement and extend support for software and tools to capture data about energy usage for mobile apps. This project is ideal for students who are interested in mobile application development.</p>"},{"location":"research/project_ideas/#energy-measurement-for-cloud","title":"Energy Measurement for Cloud","text":"<p>We are seeing a huge shift in technology and how it is being used by end-users. Almost all applications are making a shift to the cloud. This is no longer true for only services like email but for more traditional applications desktop-based applications such as word processing software. Everyday technologies that we such as Gmail, Dropbox, Office 365 are now all cloud-based. The biggest cost for cloud operators is energy usage.</p> <p>The goal of the project will be identify the current state-of-the-art in energy measurement for desktop and cloud systems and extend support for software and tools to capture energy usage in these systems. This project is ideal for someone who is interested in cloud computing.</p>"},{"location":"research/project_ideas/#smart-scheduling","title":"Smart Scheduling","text":"<p>Job shop scheduling (or job-shop problem) is an optimization problem in computer science and operations research in which ideal jobs are assigned to resources at particular times. The most basic generic version can be simplified to our Timetabling problem. At UEL within the department we schedule modules each semester taking into various constraints. Some of these constraints are:</p> <ul> <li>Ensure lecture/lab/tutorial timings don't clash</li> <li>Ensure individual students timetable don't clash</li> <li>Ensure individual academic timetable don't clash</li> <li>Ensure that students and lectures are not overworked on a particular day</li> <li>Ensure appropriate gaps between sessions for students and lecturers</li> <li>Account for specific lecturer, room and resource constraints</li> </ul> <p>This project will investigate the current practice for timetabling at the department, identify requirements of the proposed system and then develop and deploy a web application to solve the problem.</p>"},{"location":"research/project_ideas/#token-orbs","title":"Token ORBS","text":"<p>In computer programming, program slicing is the computation of the set of programs statements, the program slice, that may affect the values at some point of interest, referred to as a slicing criterion. Program slicing can be used in debugging to locate source of errors more easily. Observation-based slicing (ORBS) is a novel language-independent slicing technique that revives the original spirit of program slicing, that is, remove/delete parts of the program that does not contribute to the point of interest. A potential slice is obtained through repeated deletion which is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted.</p> <p>This project will investigate how to improve the accuracy of ORBS. This project can take one or both approaches where ORBS remains language-independent or includes language features. For the former, we can look at generic tokenizing of the code text to improve accuracy by removing at token-level rather than line-level. For the latter, we can look at tokenizing language constructs and operating on those.</p>"},{"location":"research/project_ideas/#delta-orbs","title":"Delta ORBS","text":"<p>To be defined...</p>"},{"location":"research/project_ideas/#region-connectivity","title":"Region Connectivity","text":"<p>Computation and visualisation of connectivity between the different regions of the brain is vital for many tasks such as disease identification, drug discovery and disease propagation etc. An effective visualisation can aid clinicians and biologists perform these tasks addressing a genuine research and industrial needs.</p> <p>This project will investigate and develop techniques to identify the connectivity and dispersion paths between various brain regions or similar biological structures.</p>"},{"location":"research/project_ideas/#anti-money-laundering-system","title":"Anti-Money Laundering System","text":"<p>To be defined...</p>"},{"location":"research/research_interest/","title":"Research Interest","text":""},{"location":"research/research_interest/#dependence-analysis","title":"Dependence Analysis","text":"<p>Dependence analysis is a vital part of program analysis and has applications in a wide range of software engineering activities . My research has uncovered the existence of highly inter-dependent clusters (coherent clusters) in production source code and identified reasons for their formation. These clusters have been shown to map directly to program structure, which can aid in program comprehension and understanding, as well as, identifying structural defects in code. Code Metrics &amp; Machine Learning</p> <p>I have done research into identifying unique and useful code metrics for certain applications using principal component analysis. Such analysis helps identify metics that can be used as predictors for code quality and has been successfully used in supervised learning models to predict software faults.</p>"},{"location":"research/research_interest/#dependence-clusters","title":"Dependence Clusters","text":"<p>A dependence cluster is a maximal set of program statements that are mutually inter-dependent. My research has focused on quantitative analysis to show the existence of these clusters in production source code and qualitative study of the underlying reason for their formation. My research has introduced coherent dependence clusters, a specialised form of dependence clusters. These clusters have been shown to map directly to program structure, which can aid in program comprehension and understanding, as well as, identifying structural defects in code.</p>"},{"location":"research/research_interest/#program-slicing","title":"Program Slicing","text":"<p>Current program slicing techniques cannot handle systems written in multiple programming languages. Observation-based slicing (ORBS) is a novel language-independent slicing technique capable of slicing multi-language systems including systems which contain (third party) binary components. A potential slice obtained through repeated statement deletion is validated by observing the behaviour of the program: if the slice and original program behave the same under the slicing criterion, the deletion is accepted. We have developed ORBS to take advantage of the existing build tool chains to achieve observation-based slicing. I have also created a parallel version of ORBS to reduce slicing times and make the entire process usable in practice.</p>"},{"location":"research/research_interest/#software-visualisation","title":"Software Visualisation","text":"<p>As part of my research on clusters, I have also developed a multi-level visualisation tool (deCluVi) for mapping software metrics to program statements using colour and dimension. The visualization is created from System Dependence Graphs and static backward/forward slices extracted from an industry standard static analysis tool called CodeSurfer. The dataset and the tool have been made freely available to other researchers.</p> <p>I am also working on ApiNATOMY a graphical toolkit for knowledge management with particular applications in multi-scale anatomy.</p>"},{"location":"research/research_interest/#search-based-software-engineering-sbse","title":"Search-based Software Engineering (SBSE)","text":"<p>Many software engineering problems are complex and their resolution requires unbounded resources. I am interested in the application of meta-heuristic search techniques and in particular evolutionary algorithms to these complex problems in order to find near-optimal solutions. My research has applied search-based techniques along with supervised machine learning for temporal fault-prediction and also for automatic bug triaging and assignment.</p>"},{"location":"research/visualization/","title":"Visualization","text":""},{"location":"research/visualization/#apinatomy","title":"ApiNATOMY","text":""},{"location":"research/visualization/#overview","title":"Overview","text":"<p>ApiNATOMY is an open source graphical toolkit for interactive knowledge management developed as part of the Open Physiology effort. Within ApiNATOMY, knowledge is visualized in the form of circuitboard schematics, via a platform-independent 4D (i.e. 3D + time) JavaScript environment. While the toolkit per se is generic, our driving use case is the management of knowledge about (i) multiscale anatomy in general, and (ii) routes of physiology communication in particular. A key goal of our effort is to support the open biomedical community to collaborate, share and interact with complex data and models in genomics, physiology, pharmacology and pathology.</p>"},{"location":"research/visualization/#official-website","title":"Official Website","text":"<p>The official website for ApiNATOMY can be found here The development details of ApiNATOMY can be found here</p>"},{"location":"research/visualization/#apinatomy-20","title":"ApiNATOMY 2.0","text":"<p>The ApiNATOMY Toolkit is being extended with additional abilities and have the following components:</p> <ul> <li>Graph Editor</li> <li>Lyph Editor</li> </ul>"}]}